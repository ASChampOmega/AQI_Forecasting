{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34a0e689",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:17:47.978178Z",
     "iopub.status.busy": "2025-04-10T13:17:47.977815Z",
     "iopub.status.idle": "2025-04-10T13:18:02.380502Z",
     "shell.execute_reply": "2025-04-10T13:18:02.379561Z"
    },
    "papermill": {
     "duration": 14.416983,
     "end_time": "2025-04-10T13:18:02.382489",
     "exception": false,
     "start_time": "2025-04-10T13:17:47.965506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebrae\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "import os\n",
    "\n",
    "# from polire import IDW\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from datetime import date, timedelta\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "import pickle as pkl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "705e9742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:02.415324Z",
     "iopub.status.busy": "2025-04-10T13:18:02.414516Z",
     "iopub.status.idle": "2025-04-10T13:18:02.489969Z",
     "shell.execute_reply": "2025-04-10T13:18:02.489153Z"
    },
    "papermill": {
     "duration": 0.08881,
     "end_time": "2025-04-10T13:18:02.491447",
     "exception": false,
     "start_time": "2025-04-10T13:18:02.402637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    evaluation_time_gap = 1\n",
    "    convert_numpy = False\n",
    "    target_list = ['pm2_5', 'pm10']\n",
    "    img_cols = ['lat_i', 'lon_j']\n",
    "    features = ['timeOfDay', 'lat', 'lon', 'distance', 'bus_count', 'day_of_week', 'pm2_5', 'pm10']\n",
    "\n",
    "    feature_indices = [0, 1, 2, 3, 4, 5, 7, 8]\n",
    "    mask_index = 6\n",
    "\n",
    "    batch_size = 4\n",
    "    num_epochs = 50\n",
    "\n",
    "    hidden_dim = 64\n",
    "    num_layers = 4\n",
    "    kernel_size = 3\n",
    "    dropout=0.2\n",
    "    \n",
    "    lr = 2e-3\n",
    "    patience=2\n",
    "    factor=0.7\n",
    "    warmup_steps = 15\n",
    "    total_steps = num_epochs\n",
    "    base_lr = lr\n",
    "    final_lr = 5e-6\n",
    "\n",
    "    grid_size = 30\n",
    "    day_len = 35\n",
    "    forecast_horizon = 35\n",
    "\n",
    "    model_kind=\"gru\"\n",
    "    bidirectional=False\n",
    "        \n",
    "    pm2_5_thresholds = [0, 30, 60, 90, 120, 250, 2000]\n",
    "    pm10_thresholds = [0, 50, 100, 250, 350, 430, 2000]\n",
    "    aqi_category = [\"Good\", \"Satisfactory\", \"Moderate\", \"Poor\", \"Very Poor\", \"Severe\"]\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2951d19b",
   "metadata": {
    "papermill": {
     "duration": 0.008901,
     "end_time": "2025-04-10T13:18:02.509645",
     "exception": false,
     "start_time": "2025-04-10T13:18:02.500744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ff3647a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:02.528331Z",
     "iopub.status.busy": "2025-04-10T13:18:02.528082Z",
     "iopub.status.idle": "2025-04-10T13:18:02.588436Z",
     "shell.execute_reply": "2025-04-10T13:18:02.587583Z"
    },
    "papermill": {
     "duration": 0.071582,
     "end_time": "2025-04-10T13:18:02.590178",
     "exception": false,
     "start_time": "2025-04-10T13:18:02.518596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = np.load('/kaggle/input/airdelhi-baselines-deepengineering/idw_dense_grid.npz', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab6cb99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:02.610484Z",
     "iopub.status.busy": "2025-04-10T13:18:02.610143Z",
     "iopub.status.idle": "2025-04-10T13:18:03.302500Z",
     "shell.execute_reply": "2025-04-10T13:18:03.301483Z"
    },
    "papermill": {
     "duration": 0.704399,
     "end_time": "2025-04-10T13:18:03.304337",
     "exception": false,
     "start_time": "2025-04-10T13:18:02.599938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ds.files\n",
    "df = ds['grids']\n",
    "k = ds['keys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1066d74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:03.324855Z",
     "iopub.status.busy": "2025-04-10T13:18:03.324555Z",
     "iopub.status.idle": "2025-04-10T13:18:03.393112Z",
     "shell.execute_reply": "2025-04-10T13:18:03.391860Z"
    },
    "papermill": {
     "duration": 0.08048,
     "end_time": "2025-04-10T13:18:03.394990",
     "exception": false,
     "start_time": "2025-04-10T13:18:03.314510",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rolled_df = np.roll(df, shift=35*7, axis=0)\n",
    "df = np.concatenate([df, rolled_df[:, :2]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc49e717",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:03.415729Z",
     "iopub.status.busy": "2025-04-10T13:18:03.415455Z",
     "iopub.status.idle": "2025-04-10T13:18:03.421934Z",
     "shell.execute_reply": "2025-04-10T13:18:03.420980Z"
    },
    "papermill": {
     "duration": 0.01835,
     "end_time": "2025-04-10T13:18:03.423451",
     "exception": false,
     "start_time": "2025-04-10T13:18:03.405101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2.2127020e+02, 2.4173746e+02, 0.0000000e+00, 6.0000000e+00,\n",
       "        4.4699025e-04, 0.0000000e+00, 1.0000000e+00, 2.4057486e+02,\n",
       "        2.6138818e+02], dtype=float32),\n",
       " array([2.3520129e+02, 2.5652615e+02, 0.0000000e+00, 6.0000000e+00,\n",
       "        2.1018749e-03, 0.0000000e+00, 1.0000000e+00, 2.2127020e+02,\n",
       "        2.4173746e+02], dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0, :, 0, 0], df[35*7, :, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88eec549",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:03.444157Z",
     "iopub.status.busy": "2025-04-10T13:18:03.443821Z",
     "iopub.status.idle": "2025-04-10T13:18:03.448079Z",
     "shell.execute_reply": "2025-04-10T13:18:03.447417Z"
    },
    "papermill": {
     "duration": 0.015986,
     "end_time": "2025-04-10T13:18:03.449415",
     "exception": false,
     "start_time": "2025-04-10T13:18:03.433429",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[:7*35, 7:, :, :] = np.mean(df[:7*35, :2], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5af99109",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:03.469140Z",
     "iopub.status.busy": "2025-04-10T13:18:03.468854Z",
     "iopub.status.idle": "2025-04-10T13:18:03.472022Z",
     "shell.execute_reply": "2025-04-10T13:18:03.471339Z"
    },
    "papermill": {
     "duration": 0.014508,
     "end_time": "2025-04-10T13:18:03.473340",
     "exception": false,
     "start_time": "2025-04-10T13:18:03.458832",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58bb2738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:03.492083Z",
     "iopub.status.busy": "2025-04-10T13:18:03.491841Z",
     "iopub.status.idle": "2025-04-10T13:18:03.496633Z",
     "shell.execute_reply": "2025-04-10T13:18:03.495779Z"
    },
    "papermill": {
     "duration": 0.015533,
     "end_time": "2025-04-10T13:18:03.497876",
     "exception": false,
     "start_time": "2025-04-10T13:18:03.482343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3185, 9, 30, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "209c4202",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:03.517230Z",
     "iopub.status.busy": "2025-04-10T13:18:03.517012Z",
     "iopub.status.idle": "2025-04-10T13:18:03.626485Z",
     "shell.execute_reply": "2025-04-10T13:18:03.625688Z"
    },
    "papermill": {
     "duration": 0.120898,
     "end_time": "2025-04-10T13:18:03.628097",
     "exception": false,
     "start_time": "2025-04-10T13:18:03.507199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_indices = [2, 3, 4, 5]\n",
    "def min_max_scale(d):\n",
    "    d = (d - d.min()) / (d.max() - d.min())\n",
    "    return d\n",
    "for f in feature_indices:\n",
    "    np_df[:, f, :, :] = min_max_scale(np_df[:, f, :, :])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0197350",
   "metadata": {
    "papermill": {
     "duration": 0.0097,
     "end_time": "2025-04-10T13:18:03.647728",
     "exception": false,
     "start_time": "2025-04-10T13:18:03.638028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b20de7d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:03.667471Z",
     "iopub.status.busy": "2025-04-10T13:18:03.667188Z",
     "iopub.status.idle": "2025-04-10T13:18:03.670426Z",
     "shell.execute_reply": "2025-04-10T13:18:03.669725Z"
    },
    "papermill": {
     "duration": 0.01442,
     "end_time": "2025-04-10T13:18:03.671704",
     "exception": false,
     "start_time": "2025-04-10T13:18:03.657284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# satellite_image = Image.open('/kaggle/input/delhi-satellite-images-grid/full_region_cropped.png')\n",
    "# satellite_image\n",
    "# np.array(satellite_image).shape\n",
    "# satellite_image = ToTensor()(satellite_image)\n",
    "# satellite_image = Resize(size=(300, 300))(satellite_image)\n",
    "# satellite_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1159323",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:03.691811Z",
     "iopub.status.busy": "2025-04-10T13:18:03.691585Z",
     "iopub.status.idle": "2025-04-10T13:18:03.700097Z",
     "shell.execute_reply": "2025-04-10T13:18:03.699416Z"
    },
    "papermill": {
     "duration": 0.019739,
     "end_time": "2025-04-10T13:18:03.701284",
     "exception": false,
     "start_time": "2025-04-10T13:18:03.681545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ConvImageSequenceDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, img_dict, feature_indices = None, img_cols = None, sequence_length=70, forecast_horizon=35):\n",
    "        \n",
    "        feature_indices = CFG.feature_indices if feature_indices is None else feature_indices\n",
    "        img_cols = CFG.img_cols if img_cols is None else img_cols\n",
    "        \n",
    "        self.feature_indices = feature_indices\n",
    "        self.img_dict = img_dict\n",
    "        self.img_cols = img_cols\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        self.split_df(df)\n",
    "\n",
    "        self.proc_img(img_dict)\n",
    "\n",
    "    def proc_img(self, img_dict):\n",
    "        self.imgs = []\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "\n",
    "        for i in range(CFG.grid_size):\n",
    "            # self.imgs.append([])\n",
    "            for j in range(CFG.grid_size):\n",
    "                img_file = img_dict[(i, j)]\n",
    "                img = Image.open(img_file).convert(\"RGB\")\n",
    "                tensor_img = transform(img)\n",
    "                # self.imgs[-1].append(tensor_img)\n",
    "                self.imgs.append(tensor_img)\n",
    "\n",
    "        self.imgs = torch.stack(self.imgs).to(device) # (900, 3, 32, 32)\n",
    "\n",
    "    def split_df(self, df):\n",
    "        self.X = torch.tensor(df[:, self.feature_indices, :, :]).to(device)\n",
    "        # originally, mask = 1 if it is NOT missing\n",
    "        self.masks = 1 - torch.tensor(df[:, CFG.mask_index, :, :]).to(device).unsqueeze(1) \n",
    "\n",
    "        self.total_len = len(self.X) // CFG.day_len - self.sequence_length // CFG.day_len\n",
    "        self.total_len = self.total_len - self.forecast_horizon // CFG.day_len + 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "\n",
    "    def get_index(self, i):\n",
    "        start = CFG.day_len * i\n",
    "        end   = self.sequence_length + start\n",
    "        test_end = self.forecast_horizon + end\n",
    "        return start, end, test_end\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        start, end, test_end = self.get_index(i)\n",
    "        X = self.X[start:end]# .squeeze(1)\n",
    "        y = self.X[end:test_end, :2]# .squeeze(1)\n",
    "        masks = self.masks[end:test_end]\n",
    "        return X, y, masks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cb00c1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:03.720924Z",
     "iopub.status.busy": "2025-04-10T13:18:03.720677Z",
     "iopub.status.idle": "2025-04-10T13:18:05.049705Z",
     "shell.execute_reply": "2025-04-10T13:18:05.048785Z"
    },
    "papermill": {
     "duration": 1.340612,
     "end_time": "2025-04-10T13:18:05.051415",
     "exception": false,
     "start_time": "2025-04-10T13:18:03.710803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2020-11-01 00:00:00'), Timestamp('2021-01-07 00:00:00'))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/airdelhi-baselines-deepengineering/dense_df.csv')\n",
    "\n",
    "df = df.drop(columns = 'Unnamed: 0')\n",
    "df['date_value'] = pd.to_datetime(df['date_value'])\n",
    "\n",
    "dates = pd.to_datetime([df['date_value'].min(), df['date_value'].max()])\n",
    "\n",
    "max_train_date = dates.min() + (dates.max() - dates.min()) * 0.75\n",
    "max_train_date = max_train_date.floor(\"D\")\n",
    "min_train_date = df['date_value'].min()\n",
    "max_date = df['date_value'].max().floor(\"D\")\n",
    "\n",
    "metrics_dict = {\n",
    "    'MSE': mean_squared_error, \n",
    "    'r2 score': r2_score, \n",
    "    'MAE': mean_absolute_error,\n",
    "}\n",
    "\n",
    "target_list = CFG.target_list\n",
    "\n",
    "features = ['date_value', 'timeOfDay', 'lat', 'lon', 'day_of_week', 'distance', 'bus_count']\n",
    "\n",
    "CFG.base_features = ['timeOfDay', 'lat', 'lon', 'day_of_week', 'distance', 'bus_count']\n",
    "CFG.features = CFG.base_features\n",
    "\n",
    "min_train_date, max_train_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c14720a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:05.075014Z",
     "iopub.status.busy": "2025-04-10T13:18:05.074689Z",
     "iopub.status.idle": "2025-04-10T13:18:05.087300Z",
     "shell.execute_reply": "2025-04-10T13:18:05.086576Z"
    },
    "papermill": {
     "duration": 0.024354,
     "end_time": "2025-04-10T13:18:05.088603",
     "exception": false,
     "start_time": "2025-04-10T13:18:05.064249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lat = df['lat'].unique()\n",
    "lon = df['lon'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfa2565d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:05.108721Z",
     "iopub.status.busy": "2025-04-10T13:18:05.108470Z",
     "iopub.status.idle": "2025-04-10T13:18:05.115150Z",
     "shell.execute_reply": "2025-04-10T13:18:05.114397Z"
    },
    "papermill": {
     "duration": 0.017951,
     "end_time": "2025-04-10T13:18:05.116389",
     "exception": false,
     "start_time": "2025-04-10T13:18:05.098438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0384615384615188\n",
      "0.04761904761903679\n"
     ]
    }
   ],
   "source": [
    "lat = np.sort(lat)\n",
    "lon = np.sort(lon)\n",
    "\n",
    "min_dist_lat = 1000\n",
    "for i in range(len(lat) - 1):\n",
    "    min_dist_lat = min(min_dist_lat, lat[i+1] - lat[i])\n",
    "print(min_dist_lat)\n",
    "\n",
    "for i in range(len(lat)):\n",
    "    assert (lat[i] / min_dist_lat - round(lat[i] / min_dist_lat)) < 1e-6\n",
    "\n",
    "min_dist_lon = 1000\n",
    "for i in range(len(lon) - 1):\n",
    "    min_dist_lon = min(min_dist_lon, lon[i+1] - lon[i])\n",
    "print(min_dist_lon)\n",
    "\n",
    "for i in range(len(lon)):\n",
    "    assert (lon[i] / min_dist_lon - round(lon[i] / min_dist_lon)) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01f6acc9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:05.136547Z",
     "iopub.status.busy": "2025-04-10T13:18:05.136284Z",
     "iopub.status.idle": "2025-04-10T13:18:05.141058Z",
     "shell.execute_reply": "2025-04-10T13:18:05.140375Z"
    },
    "papermill": {
     "duration": 0.016184,
     "end_time": "2025-04-10T13:18:05.142363",
     "exception": false,
     "start_time": "2025-04-10T13:18:05.126179",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21, 21)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_dimensions = round(lat[-1] / min_dist_lat), round(lon[-1] / min_dist_lon)\n",
    "grid_dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fb6a29a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:05.162578Z",
     "iopub.status.busy": "2025-04-10T13:18:05.162360Z",
     "iopub.status.idle": "2025-04-10T13:18:08.879922Z",
     "shell.execute_reply": "2025-04-10T13:18:08.879003Z"
    },
    "papermill": {
     "duration": 3.729251,
     "end_time": "2025-04-10T13:18:08.881460",
     "exception": false,
     "start_time": "2025-04-10T13:18:05.152209",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-17-5828f686307e>:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = grouped.apply(fill_na_with_previous_mean)\n"
     ]
    }
   ],
   "source": [
    "def add_lag_features(df, lags = [1]):\n",
    "\n",
    "    df = df.copy()\n",
    "    \n",
    "    df.sort_values(by=[\"lat\", \"lon\", \"timeOfDay\", \"date_value\"], inplace=True)\n",
    "\n",
    "    added_features = []\n",
    "    for l in lags:\n",
    "        df[f'pm2_5_lag_{l}'] = df.groupby(\n",
    "            ['timeOfDay', 'lat', 'lon'])['pm2_5'].shift(l)\n",
    "        df[f'pm10_lag_{l}'] = df.groupby(\n",
    "            ['timeOfDay', 'lat', 'lon'])['pm10'].shift(l)\n",
    "\n",
    "        added_features.append(f'pm2_5_lag_{l}')\n",
    "        added_features.append(f'pm10_lag_{l}')\n",
    "\n",
    "        df.sort_values(by=[\"lat\", \"lon\", \"date_value\"], inplace=True)\n",
    "\n",
    "    # Group by latitude and longitude\n",
    "    grouped = df.groupby([\"lat\", \"lon\"])\n",
    "\n",
    "    # Function to fill NaN values based on previous mean\n",
    "    def fill_na_with_previous_mean(group):\n",
    "        for col in group.columns:\n",
    "            if col not in [\"date_value\", \"lat\", \"lon\"]:\n",
    "                group[col] = group[col].astype(float)  # Ensure numeric columns\n",
    "                group[col] = group[col].fillna(group[col].expanding().mean().shift())  # Previous days' mean\n",
    "                \n",
    "                # If still NaN (first row), replace with overall mean\n",
    "                overall_mean = df[col].mean(skipna=True)\n",
    "                group[col] = group[col].fillna(overall_mean)\n",
    "        return group\n",
    "\n",
    "    # Apply the function to each group\n",
    "    df = grouped.apply(fill_na_with_previous_mean)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = df.sort_values(by = ['date_value', 'timeOfDay', 'lat', 'lon'])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    CFG.features += added_features\n",
    "    return df\n",
    "    \n",
    "df = add_lag_features(df, lags = [7])\n",
    "df = df.sort_values(by=['lat', 'lon', 'timeOfDay', 'date_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbc018be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:08.901781Z",
     "iopub.status.busy": "2025-04-10T13:18:08.901516Z",
     "iopub.status.idle": "2025-04-10T13:18:09.018920Z",
     "shell.execute_reply": "2025-04-10T13:18:09.018152Z"
    },
    "papermill": {
     "duration": 0.129282,
     "end_time": "2025-04-10T13:18:09.020608",
     "exception": false,
     "start_time": "2025-04-10T13:18:08.891326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomScaler:\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.pm2_5_max = float(df['pm2_5'].max())\n",
    "        self.pm2_5_min = float(df['pm2_5'].min())\n",
    "        \n",
    "        self.pm10_max = float(df['pm10'].max())\n",
    "        self.pm10_min = float(df['pm10'].min())\n",
    "\n",
    "    def transform_numpy(self, X, target = 'pm2_5'):\n",
    "        if target == 'pm2_5':\n",
    "            X = (X - self.pm2_5_min) / (self.pm2_5_max - self.pm2_5_min)\n",
    "        else:\n",
    "            X = (X - self.pm10_min) / (self.pm10_max - self.pm10_min)\n",
    "\n",
    "        return X\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        df['pm2_5'] = (df['pm2_5'] - self.pm2_5_min) / (self.pm2_5_max - self.pm2_5_min)\n",
    "        df['pm10'] = (df['pm10'] - self.pm10_min) / (self.pm10_max - self.pm10_min)\n",
    "        return df\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        X[:, 0] = X[:, 0] * (self.pm2_5_max - self.pm2_5_min) + self.pm2_5_min\n",
    "        X[:, 1] = X[:, 1] * (self.pm10_max - self.pm10_min) + self.pm10_min\n",
    "        return X\n",
    "\n",
    "target_scaler = CustomScaler(df)\n",
    "df = target_scaler.transform(df)\n",
    "\n",
    "CFG.features = ['timeOfDay', 'lat', 'lon', 'distance', 'bus_count', \n",
    "                'day_of_week', 'pm2_5', 'pm10', 'pm2_5_lag_7', 'pm10_lag_7']\n",
    "\n",
    "df['day_of_week'] = df['date_value'].dt.dayofweek\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[CFG.features] = scaler.fit_transform(df[CFG.features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96c409af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:09.041109Z",
     "iopub.status.busy": "2025-04-10T13:18:09.040825Z",
     "iopub.status.idle": "2025-04-10T13:18:09.085131Z",
     "shell.execute_reply": "2025-04-10T13:18:09.084109Z"
    },
    "papermill": {
     "duration": 0.055693,
     "end_time": "2025-04-10T13:18:09.086780",
     "exception": false,
     "start_time": "2025-04-10T13:18:09.031087",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np_df[:, 0, :, :] = target_scaler.transform_numpy(np_df[:, 0, :, :], 'pm2_5')\n",
    "np_df[:, 1, :, :] = target_scaler.transform_numpy(np_df[:, 1, :, :], 'pm10')\n",
    "np_df[:, 7, :, :] = target_scaler.transform_numpy(np_df[:, 7, :, :], 'pm2_5')\n",
    "np_df[:, 8, :, :] = target_scaler.transform_numpy(np_df[:, 8, :, :], 'pm10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4d3d847",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:09.106672Z",
     "iopub.status.busy": "2025-04-10T13:18:09.106378Z",
     "iopub.status.idle": "2025-04-10T13:18:09.170261Z",
     "shell.execute_reply": "2025-04-10T13:18:09.169347Z"
    },
    "papermill": {
     "duration": 0.075252,
     "end_time": "2025-04-10T13:18:09.171626",
     "exception": false,
     "start_time": "2025-04-10T13:18:09.096374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04999999999997419\n",
      "0.05263157894735837\n"
     ]
    }
   ],
   "source": [
    "with open(\"/kaggle/input/delhi-satellite-images-grid/tile_metadata.json\", 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "def process_metadata(metadata):\n",
    "    img_dict = {}\n",
    "    for k, v in metadata.items():\n",
    "        i = v['row']\n",
    "        j = v['col']\n",
    "        img_dict[(i, j)] = f\"/kaggle/input/delhi-satellite-images-grid/tiles_32x32/tile_{i}_{j}.png\"\n",
    "\n",
    "    return img_dict\n",
    "\n",
    "img_dict = process_metadata(metadata)\n",
    "\n",
    "lat = df['lat'].unique()\n",
    "lon = df['lon'].unique()\n",
    "\n",
    "lat = np.sort(lat)\n",
    "lon = np.sort(lon)\n",
    "\n",
    "min_dist_lat = 1000\n",
    "for i in range(len(lat) - 1):\n",
    "    min_dist_lat = min(min_dist_lat, lat[i+1] - lat[i])\n",
    "print(min_dist_lat)\n",
    "\n",
    "for i in range(len(lat)):\n",
    "    assert (lat[i] / min_dist_lat - round(lat[i] / min_dist_lat)) < 1e-6\n",
    "\n",
    "min_dist_lon = 1000\n",
    "for i in range(len(lon) - 1):\n",
    "    min_dist_lon = min(min_dist_lon, lon[i+1] - lon[i])\n",
    "print(min_dist_lon)\n",
    "\n",
    "for i in range(len(lon)):\n",
    "    assert (lon[i] / min_dist_lon - round(lon[i] / min_dist_lon)) < 1e-6\n",
    "\n",
    "def rounded_lat_lon(df):\n",
    "    df = df.copy()\n",
    "    df['lat_i'] = np.round(df['lat'] / min_dist_lat) + 2\n",
    "    df['lon_j'] = np.round(df['lon'] / min_dist_lon) + 4\n",
    "    return df\n",
    "\n",
    "df = rounded_lat_lon(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0543bd0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:09.194506Z",
     "iopub.status.busy": "2025-04-10T13:18:09.194243Z",
     "iopub.status.idle": "2025-04-10T13:18:09.198576Z",
     "shell.execute_reply": "2025-04-10T13:18:09.197958Z"
    },
    "papermill": {
     "duration": 0.016197,
     "end_time": "2025-04-10T13:18:09.199768",
     "exception": false,
     "start_time": "2025-04-10T13:18:09.183571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_train_index = 0\n",
    "for i in range(len(k)):\n",
    "    if k[i][0] == max_train_date:\n",
    "        max_train_index = i\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9de5d9da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:09.218874Z",
     "iopub.status.busy": "2025-04-10T13:18:09.218658Z",
     "iopub.status.idle": "2025-04-10T13:18:18.022399Z",
     "shell.execute_reply": "2025-04-10T13:18:18.021630Z"
    },
    "papermill": {
     "duration": 8.814862,
     "end_time": "2025-04-10T13:18:18.024004",
     "exception": false,
     "start_time": "2025-04-10T13:18:09.209142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = ConvImageSequenceDataset(np_df[:max_train_index, :, :, :], img_dict)\n",
    "test_ds = ConvImageSequenceDataset(np_df[max_train_index:, :, :, :], img_dict)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size = CFG.batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size = CFG.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ca0a772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.046364Z",
     "iopub.status.busy": "2025-04-10T13:18:18.046004Z",
     "iopub.status.idle": "2025-04-10T13:18:18.053043Z",
     "shell.execute_reply": "2025-04-10T13:18:18.052263Z"
    },
    "papermill": {
     "duration": 0.01986,
     "end_time": "2025-04-10T13:18:18.054496",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.034636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([70, 8, 30, 30]),\n",
       " torch.Size([35, 2, 30, 30]),\n",
       " torch.Size([35, 1, 30, 30]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt, yt, mt = train_ds[0]\n",
    "xt.shape, yt.shape, mt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e466eb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.076391Z",
     "iopub.status.busy": "2025-04-10T13:18:18.076123Z",
     "iopub.status.idle": "2025-04-10T13:18:18.081267Z",
     "shell.execute_reply": "2025-04-10T13:18:18.080381Z"
    },
    "papermill": {
     "duration": 0.017807,
     "end_time": "2025-04-10T13:18:18.082604",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.064797",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([900, 3, 32, 32])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9f68893",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.104762Z",
     "iopub.status.busy": "2025-04-10T13:18:18.104428Z",
     "iopub.status.idle": "2025-04-10T13:18:18.348566Z",
     "shell.execute_reply": "2025-04-10T13:18:18.347764Z"
    },
    "papermill": {
     "duration": 0.256798,
     "end_time": "2025-04-10T13:18:18.350038",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.093240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0392, device='cuda:0'), tensor(0.0511, device='cuda:0'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.masks.mean(), test_ds.masks.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f323900",
   "metadata": {
    "papermill": {
     "duration": 0.009854,
     "end_time": "2025-04-10T13:18:18.370338",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.360484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "562be979",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.391553Z",
     "iopub.status.busy": "2025-04-10T13:18:18.391265Z",
     "iopub.status.idle": "2025-04-10T13:18:18.413147Z",
     "shell.execute_reply": "2025-04-10T13:18:18.412450Z"
    },
    "papermill": {
     "duration": 0.034284,
     "end_time": "2025-04-10T13:18:18.414482",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.380198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageFeatureExtractor(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_tensor,\n",
    "        input_size=32,\n",
    "        output_dim=32,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super(ImageFeatureExtractor, self).__init__()\n",
    "        \n",
    "        self.input_size = 32\n",
    "        self.output_dim = output_dim\n",
    "        self.img_tensor = img_tensor.to(device) # (900, 3, 32, 32)\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3), # 30x30\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 15x15\n",
    "            nn.Conv2d(16, 32, 4), # 12x12\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 6x6\n",
    "            nn.Conv2d(32, 32, 3), # 4x4\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.MaxPool2d(2), # 2x2\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4*32, output_dim),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU()\n",
    "        ).to(device)\n",
    "    \n",
    "    def forward(self):\n",
    "        features = self.model(self.img_tensor) # (900, feature_embedding)\n",
    "        features = features.view(CFG.grid_size, CFG.grid_size, -1)\n",
    "        features = torch.permute(features, (2, 0, 1)).unsqueeze(0)\n",
    "        return features\n",
    "\n",
    "class ConvGRULayer(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, kernel_size, dilation = 1, dropout=0.2):\n",
    "        super(ConvGRULayer, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.kernel_size = kernel_size\n",
    "        padding = ((kernel_size - 1) * dilation) // 2\n",
    "\n",
    "        self.conv_ur = nn.Conv2d(\n",
    "            input_size + hidden_size, hidden_size + hidden_size, kernel_size, padding=padding,\n",
    "            dilation=dilation\n",
    "        ).to(device)\n",
    "\n",
    "        self.conv_h = nn.Conv2d(input_size + hidden_size, hidden_size, kernel_size, \n",
    "                                padding=padding, dilation=dilation).to(device)\n",
    "\n",
    "        # Dropout doesn't work when high spatial correlation between adjacent cells - use Dropout2d\n",
    "        self.dropout = nn.Dropout2d(dropout).to(device)\n",
    "        self.init_weights_()\n",
    "\n",
    "    def init_weights_(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        if h is None:\n",
    "            h = torch.zeros(B, self.hidden_size, H, W).to(device)\n",
    "\n",
    "        xh = torch.concatenate([x, h], dim = 1)\n",
    "        update_reset = self.conv_ur(xh)\n",
    "        update_reset = F.sigmoid(update_reset)\n",
    "        update = update_reset[:, :self.hidden_size]\n",
    "        reset = update_reset[:, self.hidden_size:]\n",
    "\n",
    "        x_rh = torch.concatenate([x, reset * h], dim = 1)\n",
    "        candidate_h = self.conv_h(x_rh)\n",
    "        candidate_h = F.tanh(candidate_h)\n",
    "\n",
    "        h_new = (1 - update) * h + update * candidate_h\n",
    "        h_new = self.dropout(h_new)\n",
    "\n",
    "        return h_new\n",
    "\n",
    "class ConvGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, kernel_size=3, atrous=False, dropout=0.2):\n",
    "        super(ConvGRU, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.layers = []\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                self.layers.append(\n",
    "                    ConvGRULayer(input_size, hidden_size, kernel_size, dropout=dropout).to(device)\n",
    "                )\n",
    "            else:\n",
    "                if atrous and i % 2 == 1:\n",
    "                    self.layers.append(\n",
    "                        ConvGRULayer(hidden_size, hidden_size, kernel_size, \n",
    "                                     dilation= 2, dropout=dropout).to(device)\n",
    "                    )\n",
    "                else:\n",
    "                    self.layers.append(\n",
    "                        ConvGRULayer(hidden_size, hidden_size, kernel_size, dropout=dropout).to(device)\n",
    "                    )\n",
    "\n",
    "    def init_hidden(self, x):\n",
    "        B, T, C, H, W = x.shape # batch, time, channels, h, w\n",
    "        # h_states = torch.zeros(self.num_layers, B, self.hidden_size, H, W)\n",
    "        h_states = [None] * self.num_layers\n",
    "        return h_states\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C, H, W = x.shape # batch, time, channels, h, w\n",
    "        h_states = self.init_hidden(x)\n",
    "        outputs = []\n",
    "        \n",
    "        for t in range(T):\n",
    "            xt = x[:, t]\n",
    "            for l, layer in enumerate(self.layers):\n",
    "                h_states[l] = layer(xt, h_states[l])\n",
    "                xt = h_states[l]\n",
    "\n",
    "            outputs.append(h_states[-1].unsqueeze(1))\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "class ConvPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, img_tensor, dropout=0.2,\n",
    "                 kernel_size=3, output_size=2, forecast_horizon=35):\n",
    "        \"\"\"\n",
    "        ConvGRU based model to inpaint AQI predictions\n",
    "        \"\"\"\n",
    "        super(ConvPredictor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.num_layers = num_layers\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        self.conv_gru = ConvGRU(\n",
    "            input_size, hidden_size, num_layers, kernel_size=kernel_size, atrous=False, dropout=dropout\n",
    "        ).to(device)\n",
    "\n",
    "        self.feature_extractor = ImageFeatureExtractor(img_tensor).to(device)\n",
    "        \n",
    "        self.conv_predictor = nn.Conv2d(hidden_size + 32, forecast_horizon * output_size, 1).to(device)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using Xavier for RNN and linear layers.\"\"\"\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight_ih' in name:  # Input-hidden weights\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'weight_hh' in name:  # Hidden-hidden weights\n",
    "                nn.init.orthogonal_(param)  # Orthogonal init for stability\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)  # Zero bias for stability\n",
    "            elif 'fc' in name:  # Fully connected layer\n",
    "                nn.init.kaiming_uniform_(param, nonlinearity='relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "\n",
    "        features = self.feature_extractor() # (1, 32, 30, 30)\n",
    "        features = features.repeat(B, 1, 1, 1)# (B, 32, 30, 30)\n",
    "        gru_out = self.conv_gru(x)\n",
    "        gru_final_hidden = gru_out[:, -1]\n",
    "        final_features = torch.cat([gru_final_hidden, features], dim = 1)\n",
    "        out = self.conv_predictor(final_features)\n",
    "        \n",
    "        out = out.view(B, self.forecast_horizon, self.output_size, CFG.grid_size, CFG.grid_size)\n",
    "        \n",
    "        return out\n",
    "\n",
    "def get_optimizer(model, lr = 1e-4, weight_decay = 1e-5):\n",
    "    optimizer = Adam(params = model.parameters(), \n",
    "                     lr = lr, weight_decay = weight_decay)\n",
    "    return optimizer\n",
    "\n",
    "def get_reduce_lr(optimizer, factor=0.1, patience=2):\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode = 'min', factor = factor, patience = patience)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7633a30d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.438488Z",
     "iopub.status.busy": "2025-04-10T13:18:18.438211Z",
     "iopub.status.idle": "2025-04-10T13:18:18.442695Z",
     "shell.execute_reply": "2025-04-10T13:18:18.441964Z"
    },
    "papermill": {
     "duration": 0.018995,
     "end_time": "2025-04-10T13:18:18.444031",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.425036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(\n",
    "    img_tensor, \n",
    "    input_size=None, \n",
    "    hidden_size=32, \n",
    "    num_layers=3, \n",
    "    dropout=0.2,\n",
    "    kernel_size=3, \n",
    "    output_size=2, \n",
    "    forecast_horizon=35\n",
    "):\n",
    "    if input_size is None:\n",
    "        input_size = len(CFG.feature_indices)\n",
    "        \n",
    "    model = ConvPredictor(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        output_size=output_size, \n",
    "        forecast_horizon=forecast_horizon,\n",
    "        img_tensor = img_tensor\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3550e5d4",
   "metadata": {
    "papermill": {
     "duration": 0.010539,
     "end_time": "2025-04-10T13:18:18.465444",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.454905",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b71165e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.486622Z",
     "iopub.status.busy": "2025-04-10T13:18:18.486298Z",
     "iopub.status.idle": "2025-04-10T13:18:18.493748Z",
     "shell.execute_reply": "2025-04-10T13:18:18.493032Z"
    },
    "papermill": {
     "duration": 0.019665,
     "end_time": "2025-04-10T13:18:18.495023",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.475358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MaskedMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, preds, targets, mask):\n",
    "        loss = (preds - targets) ** 2\n",
    "        masked_loss = (loss * mask).sum() / (mask.sum() + self.eps)\n",
    "        return masked_loss\n",
    "\n",
    "class MaskedMAELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(MaskedMAELoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, preds, targets, mask):\n",
    "        loss = abs(preds - targets)\n",
    "        masked_loss = (loss * mask).sum() / (mask.sum() + self.eps)\n",
    "        return masked_loss\n",
    "\n",
    "class MaskedR2(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(MaskedR2, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, preds, targets, mask):\n",
    "        error = ((targets - preds) ** 2) * mask\n",
    "        masked_mse = error.sum() / (mask.sum() + self.eps)\n",
    "        mean_y = (targets * mask).sum() / (mask.sum() + self.eps)\n",
    "        total_var = (((targets - mean_y) ** 2) * mask).sum() / (mask.sum() + self.eps)\n",
    "        r2 = 1 - (masked_mse / (total_var + self.eps))\n",
    "        return r2\n",
    "\n",
    "masked_metrics_dict = {\n",
    "    'MSE': MaskedMSELoss(),\n",
    "    'MAE': MaskedMAELoss(),\n",
    "    'r2 score': MaskedR2(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ef4dc80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.516875Z",
     "iopub.status.busy": "2025-04-10T13:18:18.516623Z",
     "iopub.status.idle": "2025-04-10T13:18:18.522507Z",
     "shell.execute_reply": "2025-04-10T13:18:18.521832Z"
    },
    "papermill": {
     "duration": 0.018373,
     "end_time": "2025-04-10T13:18:18.523808",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.505435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RunningLoss:\n",
    "    def __init__(self, window = 10):\n",
    "        self.loss = 0\n",
    "        self.total = 0\n",
    "        self.loss_last = []\n",
    "        self.total_last = []\n",
    "\n",
    "        self.window = window\n",
    "    \n",
    "    def update(self, loss, batch_size):\n",
    "        total = 1\n",
    "        self.loss += loss\n",
    "        self.total += 1\n",
    "        \n",
    "        self.loss_last.append(loss)\n",
    "        self.total_last.append(total)\n",
    "        if len(self.loss_last) > self.window:\n",
    "            self.loss_last.pop(0)\n",
    "            self.total_last.pop(0)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.loss = 0\n",
    "        self.total = 0\n",
    "        self.loss_last = []\n",
    "        self.total_last = []\n",
    "\n",
    "    def print(self):\n",
    "        print(f\"Accuracy: {self.loss / self.total}\")\n",
    "    \n",
    "    def get_curr_stats(self):\n",
    "        return sum(self.loss_last) / sum(self.total_last)\n",
    "\n",
    "    def total_stats(self):\n",
    "        return self.loss / self.total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb5d7096",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.545119Z",
     "iopub.status.busy": "2025-04-10T13:18:18.544824Z",
     "iopub.status.idle": "2025-04-10T13:18:18.551483Z",
     "shell.execute_reply": "2025-04-10T13:18:18.550790Z"
    },
    "papermill": {
     "duration": 0.018838,
     "end_time": "2025-04-10T13:18:18.552800",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.533962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(\n",
    "    model,\n",
    "    test_dl,\n",
    "    criterion,\n",
    "    verbose=False,\n",
    "    get_predict=False,\n",
    "):\n",
    "    running_loss = RunningLoss()\n",
    "\n",
    "    predict_array = []\n",
    "    true_array = []\n",
    "    mask_array = []\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        pbar = tqdm(total = len(test_dl), ncols = 110, desc = \"Validation Progress\") # , dynamic_ncols=True, leave=False)\n",
    "    else:\n",
    "        pbar = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y, mask) in enumerate(test_dl):\n",
    "            x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "            pred = model(x)\n",
    "\n",
    "            pred = target_scaler.inverse_transform(pred)\n",
    "            y = target_scaler.inverse_transform(y)\n",
    "\n",
    "            temp_loss = criterion(pred, y, mask)\n",
    "            running_loss.update(temp_loss.item(), batch_size=x.shape[0])\n",
    "\n",
    "            if get_predict:\n",
    "                predict_array.append(pred)\n",
    "                true_array.append(y)\n",
    "                mask_array.append(mask)\n",
    "\n",
    "            if verbose >= 1:\n",
    "                total = len(test_dl)\n",
    "                curr_loss = np.sqrt(running_loss.get_curr_stats())\n",
    "                pbar.set_description(f\"Validation Step {i} / {total}\")\n",
    "                pbar.set_postfix(Loss=f\"{curr_loss:.4f}\")\n",
    "                pbar.update(1)\n",
    "        if verbose >= 1:\n",
    "            pbar.close()\n",
    "\n",
    "    if get_predict:\n",
    "        return predict_array, true_array, mask_array, np.sqrt(running_loss.total_stats())\n",
    "    \n",
    "    return np.sqrt(running_loss.total_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09347aa3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.574791Z",
     "iopub.status.busy": "2025-04-10T13:18:18.574480Z",
     "iopub.status.idle": "2025-04-10T13:18:18.587421Z",
     "shell.execute_reply": "2025-04-10T13:18:18.586636Z"
    },
    "papermill": {
     "duration": 0.02577,
     "end_time": "2025-04-10T13:18:18.588774",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.563004",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        num_epochs,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        scheduler=None,\n",
    "        do_validate=False,\n",
    "        validate_frequency=1,\n",
    "        verbose=True,\n",
    "        metrics_dict=None,\n",
    "        model_name=\"conv_\"\n",
    "):\n",
    "    if metrics_dict is None:\n",
    "        metrics_dict = masked_metrics_dict\n",
    "    \n",
    "    loss_list = []\n",
    "    val_loss_list = []\n",
    "    all_loss_list = []\n",
    "\n",
    "    best_val_loss = 100000000\n",
    "    best_val_loss25 = 100000000\n",
    "    best_val_loss10 = 100000000\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), disable=verbose >= -1):\n",
    "        running_loss = RunningLoss()\n",
    "        if verbose >= 1:\n",
    "            pbar = tqdm(total = len(train_dataloader), ncols = 110, desc = \"Training Progress\") # , dynamic_ncols=True, leave=False)\n",
    "        else:\n",
    "            pbar = None\n",
    "        for i, (x, y, mask) in enumerate(train_dataloader):\n",
    "            x, y, mask = x.to(device), y.to(device), mask.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y, mask)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            all_loss_list.append(loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = target_scaler.inverse_transform(pred)\n",
    "                y = target_scaler.inverse_transform(y)\n",
    "                temp_loss = torch.sqrt(criterion(pred, y, mask))\n",
    "                running_loss.update(temp_loss.item(), batch_size=x.shape[0])\n",
    "\n",
    "            if verbose >= 1:\n",
    "                total = len(train_dataloader)\n",
    "                curr_loss = running_loss.get_curr_stats()\n",
    "                pbar.set_description(f\"Epoch {epoch}: Step {i} / {total}\")\n",
    "                pbar.set_postfix(Loss=f\"{curr_loss:.4f}\")\n",
    "                pbar.update(1)\n",
    "        if verbose >= 1:\n",
    "            print(f\"Train Loss Total: {running_loss.total_stats()}\")\n",
    "            pbar.close()\n",
    "        \n",
    "        if do_validate and epoch % validate_frequency == validate_frequency - 1:\n",
    "            pred1, y1, mask1, val_loss = validate(model, val_dataloader, criterion, \n",
    "                                           verbose=verbose, get_predict=True)\n",
    "            pred1 = torch.concat(pred1)\n",
    "            y1 = torch.concat(y1)\n",
    "            mask1 = torch.concat(mask1)\n",
    "            \n",
    "            val_loss_list.append(val_loss)\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(val_loss)\n",
    "            \n",
    "            if verbose >= 1:\n",
    "                print(f\"Val Loss Total: {val_loss}\")\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                if verbose >= 0:\n",
    "                    print(f\"Better Val Loss: {val_loss} < {best_val_loss}\")\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), f\"./{model_name}best.pth\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, t in enumerate(CFG.target_list):\n",
    "                    for n, m in metrics_dict.items():\n",
    "                        l = m(pred1[:, :, i, :, :], y1[:, :, i, :, :], mask1[:, :, 0, :, :]).cpu().detach()\n",
    "                        l = float(l)\n",
    "                        if n == 'MSE':\n",
    "                            # print(y1[:, :, i].shape, pred1[:, :, i].shape, mask1.shape)\n",
    "                            mse_loss = np.sqrt(l)\n",
    "                            if verbose >= 1:\n",
    "                                print(f\"{t}: {n}: {mse_loss}\")\n",
    "                            \n",
    "                            if i == 0 and mse_loss < best_val_loss25:\n",
    "                                if verbose >= 0:\n",
    "                                    print(f\"----------Better MSE on pm2_5 {mse_loss} < {best_val_loss25}\")\n",
    "                                torch.save(model.state_dict(), f\"./{model_name}2_5.pth\")\n",
    "                                best_val_loss25 = mse_loss\n",
    "                            if i == 1 and mse_loss < best_val_loss10:\n",
    "                                if verbose >= 0:\n",
    "                                    print(f\"----------Better MSE on pm10 {mse_loss} < {best_val_loss10}\")\n",
    "                                torch.save(model.state_dict(), f\"./{model_name}10.pth\")\n",
    "                                best_val_loss10 = mse_loss\n",
    "                        if verbose >= 1:\n",
    "                            if n == 'MSE':\n",
    "                                l = np.sqrt(l)\n",
    "                                l = float(l)\n",
    "                            print(f\"{t}: {n}: {l}\")\n",
    "        \n",
    "        epoch_loss = running_loss.total_stats()\n",
    "        loss_list.append(epoch_loss)\n",
    "    \n",
    "    return loss_list, val_loss_list, all_loss_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb1f503",
   "metadata": {
    "papermill": {
     "duration": 0.054433,
     "end_time": "2025-04-10T13:18:18.653634",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.599201",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ccd8a1a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.675524Z",
     "iopub.status.busy": "2025-04-10T13:18:18.675201Z",
     "iopub.status.idle": "2025-04-10T13:18:18.680353Z",
     "shell.execute_reply": "2025-04-10T13:18:18.679676Z"
    },
    "papermill": {
     "duration": 0.017357,
     "end_time": "2025-04-10T13:18:18.681595",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.664238",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SafeSchedulerWrapper:\n",
    "    def __init__(self, scheduler):\n",
    "        self.scheduler = scheduler\n",
    "\n",
    "    def step(self, metric=None):\n",
    "        self.scheduler.step()\n",
    "\n",
    "def cosine_scheduler_with_warmup(optimizer, warmup_steps, total_steps, base_lr, final_lr=0.0):\n",
    "    def lr_lambda(current_step):\n",
    "        if current_step < warmup_steps:\n",
    "            return current_step / warmup_steps\n",
    "        progress = (current_step - warmup_steps) / (total_steps - warmup_steps)\n",
    "        cosine_decay = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return final_lr / base_lr + (1 - final_lr / base_lr) * cosine_decay\n",
    "\n",
    "    return SafeSchedulerWrapper(torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "df56604c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.702472Z",
     "iopub.status.busy": "2025-04-10T13:18:18.702194Z",
     "iopub.status.idle": "2025-04-10T13:18:18.753002Z",
     "shell.execute_reply": "2025-04-10T13:18:18.752251Z"
    },
    "papermill": {
     "duration": 0.062916,
     "end_time": "2025-04-10T13:18:18.754653",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.691737",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_model(\n",
    "    img_tensor=train_ds.imgs,\n",
    "    hidden_size = CFG.hidden_dim,\n",
    "    num_layers=CFG.num_layers, \n",
    "    dropout=CFG.dropout,\n",
    "    kernel_size=CFG.kernel_size, \n",
    "    output_size=2, \n",
    "    forecast_horizon=CFG.forecast_horizon\n",
    ").to(device)\n",
    "\n",
    "criterion = MaskedMSELoss().to(device)\n",
    "optimizer = get_optimizer(model, lr = CFG.lr)\n",
    "scheduler = get_reduce_lr(optimizer, factor=CFG.factor, patience=CFG.patience)\n",
    "# cosine_scheduler_with_warmup = cosine_scheduler_with_warmup(\n",
    "#     optimizer, CFG.warmup_steps, CFG.total_steps, CFG.base_lr, CFG.final_lr\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c719b1e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.776547Z",
     "iopub.status.busy": "2025-04-10T13:18:18.776249Z",
     "iopub.status.idle": "2025-04-10T13:18:18.779414Z",
     "shell.execute_reply": "2025-04-10T13:18:18.778774Z"
    },
    "papermill": {
     "duration": 0.015411,
     "end_time": "2025-04-10T13:18:18.780675",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.765264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# validate(model, test_dl, criterion, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f648ab7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.802019Z",
     "iopub.status.busy": "2025-04-10T13:18:18.801757Z",
     "iopub.status.idle": "2025-04-10T13:18:18.804998Z",
     "shell.execute_reply": "2025-04-10T13:18:18.804159Z"
    },
    "papermill": {
     "duration": 0.015371,
     "end_time": "2025-04-10T13:18:18.806383",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.791012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # loss_list, val_loss_list, all_loss_list\n",
    "# # pred1, y1, \n",
    "# loss_list, val_loss_list, all_loss_list = train(\n",
    "#     model,\n",
    "#     criterion,\n",
    "#     optimizer,\n",
    "#     num_epochs=CFG.num_epochs,\n",
    "#     train_dataloader=train_dl,\n",
    "#     val_dataloader=test_dl,\n",
    "#     scheduler=scheduler,\n",
    "#     do_validate=True,\n",
    "#     verbose=-2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f834a2ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.828019Z",
     "iopub.status.busy": "2025-04-10T13:18:18.827716Z",
     "iopub.status.idle": "2025-04-10T13:18:18.834445Z",
     "shell.execute_reply": "2025-04-10T13:18:18.833809Z"
    },
    "papermill": {
     "duration": 0.019141,
     "end_time": "2025-04-10T13:18:18.835932",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.816791",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_aqi_category_indices(preds, thresholds):\n",
    "    indices = torch.bucketize(preds, torch.tensor(thresholds).to(device), right=False) - 1\n",
    "    return torch.clamp(indices, min=0, max=5)\n",
    "\n",
    "def compute_aqi_classification_metrics(pred, label, mask):\n",
    "    mask = mask.bool().squeeze(2)\n",
    "\n",
    "    metrics = {}\n",
    "    for i, (name, thresholds) in enumerate(zip(CFG.target_list, [CFG.pm2_5_thresholds, CFG.pm10_thresholds])):\n",
    "        pred_class = get_aqi_category_indices(pred[:, :, i, :, :], thresholds)\n",
    "        label_class = get_aqi_category_indices(label[:, :, i, :, :], thresholds)\n",
    "\n",
    "        pred_class = pred_class[mask]\n",
    "        label_class = label_class[mask]\n",
    "\n",
    "        class_metrics = {}\n",
    "        for class_idx, class_name in enumerate(CFG.aqi_category):\n",
    "            true_pos = ((pred_class == class_idx) & (label_class == class_idx)).sum().item()\n",
    "            total_pred = (pred_class == class_idx).sum().item()\n",
    "            total_true = (label_class == class_idx).sum().item()\n",
    "\n",
    "            accuracy = true_pos / total_true if total_true else 0.0\n",
    "            precision = true_pos / total_pred if total_pred else 0.0\n",
    "            recall = true_pos / total_true if total_true else 0.0\n",
    "\n",
    "            class_metrics[class_name] = {\n",
    "                \"true\": true_pos,\n",
    "                \"total\": total_true,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall\n",
    "            }\n",
    "\n",
    "        metrics[name] = class_metrics\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e40c05c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.856957Z",
     "iopub.status.busy": "2025-04-10T13:18:18.856658Z",
     "iopub.status.idle": "2025-04-10T13:18:18.862532Z",
     "shell.execute_reply": "2025-04-10T13:18:18.861697Z"
    },
    "papermill": {
     "duration": 0.01769,
     "end_time": "2025-04-10T13:18:18.863848",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.846158",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_eval(model, test_dl, criterion, verbose=True):\n",
    "    o = validate(model, test_dl, criterion, verbose=verbose, get_predict=True)\n",
    "    pred = torch.concat(o[0])\n",
    "    y = torch.concat(o[1])\n",
    "    mask = torch.concat(o[2])\n",
    "    model_performance = {'pm2_5': {}, 'pm10': {}}\n",
    "    for i, t in enumerate(CFG.target_list):\n",
    "        for n, m in masked_metrics_dict.items():\n",
    "            l = m(pred[:, :, i, :, :], y[:, :, i, :, :], mask[:, :, 0, :, :])\n",
    "            if n == 'MSE':\n",
    "                l = np.sqrt(l.cpu().numpy())\n",
    "                if verbose:\n",
    "                    print(f\"{t}: {n}: {l}\")\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"{t}: {n}: {l}\")\n",
    "\n",
    "            model_performance[t][n] = float(l)\n",
    "    \n",
    "    model_performance['classification'] = compute_aqi_classification_metrics(pred, y, mask)\n",
    "    \n",
    "    return model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e8d69c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.884572Z",
     "iopub.status.busy": "2025-04-10T13:18:18.884299Z",
     "iopub.status.idle": "2025-04-10T13:18:18.887477Z",
     "shell.execute_reply": "2025-04-10T13:18:18.886770Z"
    },
    "papermill": {
     "duration": 0.014927,
     "end_time": "2025-04-10T13:18:18.888776",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.873849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_sd = torch.load('/kaggle/working/conv_best.pth')\n",
    "# model.load_state_dict(model_sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6cfbd1dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.910827Z",
     "iopub.status.busy": "2025-04-10T13:18:18.910509Z",
     "iopub.status.idle": "2025-04-10T13:18:18.913684Z",
     "shell.execute_reply": "2025-04-10T13:18:18.913006Z"
    },
    "papermill": {
     "duration": 0.015737,
     "end_time": "2025-04-10T13:18:18.915086",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.899349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_eval(model, train_dl, criterion, verbose=True)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e74ccd",
   "metadata": {
    "papermill": {
     "duration": 0.009752,
     "end_time": "2025-04-10T13:18:18.935101",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.925349",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Equivalent Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd7efed4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:18.955639Z",
     "iopub.status.busy": "2025-04-10T13:18:18.955300Z",
     "iopub.status.idle": "2025-04-10T13:18:21.745617Z",
     "shell.execute_reply": "2025-04-10T13:18:21.744536Z"
    },
    "papermill": {
     "duration": 2.802907,
     "end_time": "2025-04-10T13:18:21.747569",
     "exception": false,
     "start_time": "2025-04-10T13:18:18.944662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_ds = np.load('/kaggle/input/airdelhi-baselines-deepengineering/idw_full_dense_grid.npz', allow_pickle=True)\n",
    "\n",
    "full_df = full_ds['grids']\n",
    "full_k = full_ds['keys']\n",
    "\n",
    "rolled_df = np.roll(full_df, shift=35*7, axis=0)\n",
    "full_df = np.concatenate([full_df, rolled_df[:, :2]], axis = 1)\n",
    "full_df[:7*35, 7:, :, :] = np.mean(full_df[:7*35, :2], axis = 0)\n",
    "\n",
    "full_np_df = full_df\n",
    "full_np_df[:, 0, :, :] = target_scaler.transform_numpy(full_np_df[:, 0, :, :], 'pm2_5')\n",
    "full_np_df[:, 1, :, :] = target_scaler.transform_numpy(full_np_df[:, 1, :, :], 'pm10')\n",
    "full_np_df[:, 7, :, :] = target_scaler.transform_numpy(full_np_df[:, 7, :, :], 'pm2_5')\n",
    "full_np_df[:, 8, :, :] = target_scaler.transform_numpy(full_np_df[:, 8, :, :], 'pm10')\n",
    "\n",
    "feature_indices = [2, 3, 4, 5]\n",
    "def min_max_scale(d):\n",
    "    d = (d - d.min()) / (d.max() - d.min())\n",
    "    return d\n",
    "\n",
    "for f in feature_indices:\n",
    "    full_np_df[:, f, :, :] = min_max_scale(full_np_df[:, f, :, :])    \n",
    "\n",
    "full_train_ds = ConvImageSequenceDataset(full_np_df[:max_train_index, :, :, :], img_dict)\n",
    "full_test_ds = ConvImageSequenceDataset(full_np_df[max_train_index:, :, :, :], img_dict)\n",
    "\n",
    "full_train_dl = DataLoader(full_train_ds, batch_size = CFG.batch_size, shuffle=True)\n",
    "full_test_dl = DataLoader(full_test_ds, batch_size = CFG.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb08411",
   "metadata": {
    "papermill": {
     "duration": 0.010429,
     "end_time": "2025-04-10T13:18:21.769264",
     "exception": false,
     "start_time": "2025-04-10T13:18:21.758835",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ablation Study"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c12d591",
   "metadata": {
    "papermill": {
     "duration": 0.010072,
     "end_time": "2025-04-10T13:18:21.789913",
     "exception": false,
     "start_time": "2025-04-10T13:18:21.779841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ablation 1: Model Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "227348d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:21.811270Z",
     "iopub.status.busy": "2025-04-10T13:18:21.810949Z",
     "iopub.status.idle": "2025-04-10T13:18:21.819397Z",
     "shell.execute_reply": "2025-04-10T13:18:21.818620Z"
    },
    "papermill": {
     "duration": 0.020831,
     "end_time": "2025-04-10T13:18:21.820691",
     "exception": false,
     "start_time": "2025-04-10T13:18:21.799860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "\n",
    "def run_grid_search(\n",
    "    param_grid: dict,\n",
    "        df_dict,\n",
    "        model_name,\n",
    "        img_tensor,\n",
    "    model_save_dir: str,\n",
    "    result_json_path: str,\n",
    "    default_params: dict = None\n",
    "):\n",
    "    from copy import deepcopy\n",
    "\n",
    "    # Generate all parameter combinations\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    \n",
    "    param_combinations = [\n",
    "        {**(default_params or {}), **dict(zip(keys, v))}\n",
    "        for v in itertools.product(*values)\n",
    "    ]\n",
    "    \n",
    "    model_dict = {}\n",
    "    results_list = []\n",
    "    \n",
    "    for idx, param_set in enumerate(param_combinations):\n",
    "        print(f\"Training model {idx} with params: {param_set}\")\n",
    "        \n",
    "        model = get_model(img_tensor=img_tensor, **param_set)\n",
    "        criterion = MaskedMSELoss().to(device)\n",
    "        optimizer = get_optimizer(model, lr = CFG.lr)\n",
    "        scheduler = get_reduce_lr(optimizer, factor=CFG.factor, patience=CFG.patience)\n",
    "        \n",
    "        loss_list, val_loss_list, all_loss_list = train(\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            num_epochs=CFG.num_epochs,\n",
    "            train_dataloader=df_dict['train_dl'],\n",
    "            val_dataloader=df_dict['test_dl'],\n",
    "            scheduler=scheduler,\n",
    "            do_validate=True,\n",
    "            validate_frequency=1,\n",
    "            verbose=-2,\n",
    "            model_name=model_save_dir + f\"{model_name}_{idx}_\"\n",
    "        )\n",
    "        \n",
    "        model_path = model_save_dir + f\"final_{model_name}_{idx}.pt\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        model_dict[idx] = model\n",
    "\n",
    "        result_metrics = {}\n",
    "        paths = ['best', '2_5', '10']\n",
    "        for p in paths:\n",
    "            result_metrics[p] = {}\n",
    "            model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            \n",
    "            for df_name, df1 in df_dict.items():\n",
    "                result_metrics[p][df_name] = predict_eval(model, df1, criterion, verbose=False)\n",
    "\n",
    "        results_list.append({\n",
    "            \"model_index\": idx,\n",
    "            \"model_name\": model_path,\n",
    "            \"parameters\": deepcopy(param_set),\n",
    "            \"results\": result_metrics\n",
    "        })\n",
    "\n",
    "    with open(result_json_path, \"w\") as f:\n",
    "        json.dump(results_list, f, indent=4)\n",
    "\n",
    "    return model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16263e9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:18:21.844148Z",
     "iopub.status.busy": "2025-04-10T13:18:21.843797Z",
     "iopub.status.idle": "2025-04-10T13:44:50.034843Z",
     "shell.execute_reply": "2025-04-10T13:44:50.034005Z"
    },
    "papermill": {
     "duration": 1588.204646,
     "end_time": "2025-04-10T13:44:50.036646",
     "exception": false,
     "start_time": "2025-04-10T13:18:21.832000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 0 with params: {'input_size': None, 'forecast_horizon': 35, 'hidden_size': 64, 'num_layers': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [01:32<00:00,  1.85s/it]\n",
      "<ipython-input-41-240d09f39090>:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n",
      "<ipython-input-36-836d6a5d92c0>:2: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at ../aten/src/ATen/native/BucketizationUtils.h:32.)\n",
      "  indices = torch.bucketize(preds, torch.tensor(thresholds).to(device), right=False) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 with params: {'input_size': None, 'forecast_horizon': 35, 'hidden_size': 64, 'num_layers': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [03:59<00:00,  4.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 2 with params: {'input_size': None, 'forecast_horizon': 35, 'hidden_size': 64, 'num_layers': 6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [07:53<00:00,  9.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 3 with params: {'input_size': None, 'forecast_horizon': 35, 'hidden_size': 64, 'num_layers': 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [11:47<00:00, 14.16s/it]\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    # 'hidden_size' : [16, 32, 64, 128], \n",
    "    'hidden_size' : [64], \n",
    "    'num_layers' : [1, 3, 6, 9],\n",
    "}\n",
    "\n",
    "default_params = {\n",
    "    'input_size' : None, \n",
    "    'forecast_horizon' : 35\n",
    "}\n",
    "\n",
    "df_dict = {\n",
    "    'train_dl' : train_dl,\n",
    "    'test_dl' : test_dl,\n",
    "    'full_train_dl' : full_train_dl,\n",
    "    'full_test_dl' : full_test_dl,\n",
    "}\n",
    "\n",
    "model_name = \"model_size_test\"\n",
    "\n",
    "model_save_dir = f\"./{model_name}_dir/\"\n",
    "result_json_path = f\"{model_save_dir}{model_name}_results.json\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "img_tensor = train_ds.imgs\n",
    "\n",
    "model_dict = run_grid_search(\n",
    "    param_grid,\n",
    "        df_dict,\n",
    "        model_name,\n",
    "        img_tensor,\n",
    "    model_save_dir,\n",
    "    result_json_path,\n",
    "    default_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c3910d",
   "metadata": {
    "papermill": {
     "duration": 0.021739,
     "end_time": "2025-04-10T13:44:50.080779",
     "exception": false,
     "start_time": "2025-04-10T13:44:50.059040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ablation 2: Sequence Length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f1d4822b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:44:50.122473Z",
     "iopub.status.busy": "2025-04-10T13:44:50.122117Z",
     "iopub.status.idle": "2025-04-10T13:44:50.126648Z",
     "shell.execute_reply": "2025-04-10T13:44:50.125723Z"
    },
    "papermill": {
     "duration": 0.0274,
     "end_time": "2025-04-10T13:44:50.128020",
     "exception": false,
     "start_time": "2025-04-10T13:44:50.100620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dl(df, sequence_length=35, forecast_horizon=35):\n",
    "    ds = ConvImageSequenceDataset(df, \n",
    "                              sequence_length = sequence_length, forecast_horizon = forecast_horizon,\n",
    "                             img_dict=img_dict)\n",
    "    dl = DataLoader(ds, batch_size = CFG.batch_size, shuffle=True)\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8307458e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:44:50.172040Z",
     "iopub.status.busy": "2025-04-10T13:44:50.171634Z",
     "iopub.status.idle": "2025-04-10T13:44:50.181541Z",
     "shell.execute_reply": "2025-04-10T13:44:50.180583Z"
    },
    "papermill": {
     "duration": 0.033692,
     "end_time": "2025-04-10T13:44:50.182977",
     "exception": false,
     "start_time": "2025-04-10T13:44:50.149285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_grid_search_df(\n",
    "    param_grid: dict,\n",
    "        df_dict,\n",
    "        model_name,\n",
    "        img_tensor,\n",
    "    model_save_dir: str,\n",
    "    result_json_path: str,\n",
    "    default_params: dict = None,\n",
    "    model_params: dict = None,\n",
    "):\n",
    "    from copy import deepcopy\n",
    "\n",
    "    # Generate all parameter combinations\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    \n",
    "    param_combinations = [\n",
    "        {**(default_params or {}), **dict(zip(keys, v))}\n",
    "        for v in itertools.product(*values)\n",
    "    ]\n",
    "    \n",
    "    model_dict = {}\n",
    "    results_list = []\n",
    "    \n",
    "    for idx, param_set in enumerate(param_combinations):\n",
    "        print(f\"Training model on dataset {idx} with params: {param_set}\")\n",
    "\n",
    "        model = get_model(img_tensor = img_tensor, **model_params)\n",
    "\n",
    "        ds_dict = {\n",
    "            k : get_dl(df = v, **param_set)\n",
    "            for k, v in df_dict.items()\n",
    "        }\n",
    "        \n",
    "        criterion = MaskedMSELoss().to(device)\n",
    "        optimizer = get_optimizer(model, lr = CFG.lr)\n",
    "        scheduler = get_reduce_lr(optimizer, factor=CFG.factor, patience=CFG.patience)\n",
    "        \n",
    "        loss_list, val_loss_list, all_loss_list = train(\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            num_epochs=CFG.num_epochs,\n",
    "            train_dataloader=ds_dict['train_dl'],\n",
    "            val_dataloader=ds_dict['test_dl'],\n",
    "            scheduler=scheduler,\n",
    "            do_validate=True,\n",
    "            validate_frequency=1,\n",
    "            verbose=-2,\n",
    "            model_name=model_save_dir + f\"{model_name}_{idx}_\"\n",
    "        )\n",
    "        \n",
    "        model_path = model_save_dir + f\"final_{model_name}_{idx}.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        model_dict[idx] = model\n",
    "        \n",
    "        result_metrics = {}\n",
    "        paths = ['best', '2_5', '10']\n",
    "        for p in paths:\n",
    "            result_metrics[p] = {}\n",
    "            model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            \n",
    "            for df_name, df1 in ds_dict.items():\n",
    "                result_metrics[p][df_name] = predict_eval(model, df1, criterion, verbose=False)\n",
    "\n",
    "        results_list.append({\n",
    "            \"model_index\": idx,\n",
    "            \"model_name\": model_path,\n",
    "            \"parameters\": deepcopy(param_set),\n",
    "            \"results\": result_metrics\n",
    "        })\n",
    "\n",
    "    with open(result_json_path, \"w\") as f:\n",
    "        json.dump(results_list, f, indent=4)\n",
    "\n",
    "    return model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02bc84f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T13:44:50.227008Z",
     "iopub.status.busy": "2025-04-10T13:44:50.226567Z",
     "iopub.status.idle": "2025-04-10T14:15:29.632701Z",
     "shell.execute_reply": "2025-04-10T14:15:29.631965Z"
    },
    "papermill": {
     "duration": 1839.430287,
     "end_time": "2025-04-10T14:15:29.634454",
     "exception": false,
     "start_time": "2025-04-10T13:44:50.204167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 0 with params: {'sequence_length': 35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [02:12<00:00,  2.65s/it]\n",
      "<ipython-input-44-98c7dc03ae99>:60: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 1 with params: {'sequence_length': 70}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [04:08<00:00,  4.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 2 with params: {'sequence_length': 105}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [05:44<00:00,  6.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 3 with params: {'sequence_length': 140}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [07:28<00:00,  8.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 4 with params: {'sequence_length': 175}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [09:14<00:00, 11.09s/it]\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'sequence_length' : [35, 70, 105, 140, 175], \n",
    "}\n",
    "\n",
    "default_params = {\n",
    "    'input_size' : None, \n",
    "    'forecast_horizon' : 35\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    'input_size': None,\n",
    "    'forecast_horizon': 35,\n",
    "    'num_layers': 3,\n",
    "    'hidden_size': 64,\n",
    "}\n",
    "\n",
    "df_dict = {\n",
    "    'train_dl' : np_df[:max_train_index, :, :, :],\n",
    "    'test_dl' : np_df[max_train_index:, :, :, :],\n",
    "    'full_train_dl' : full_np_df[:max_train_index, :, :, :],\n",
    "    'full_test_dl' : full_np_df[max_train_index:, :, :, :],\n",
    "}\n",
    "\n",
    "model_name = \"sequence_length_test\"\n",
    "\n",
    "model_save_dir = f\"./{model_name}_dir/\"\n",
    "result_json_path = f\"{model_save_dir}{model_name}_results.json\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "model_dict1 = run_grid_search_df(\n",
    "    param_grid,\n",
    "        df_dict,\n",
    "        model_name,\n",
    "        img_tensor,\n",
    "    model_save_dir,\n",
    "    result_json_path,\n",
    "    default_params=None,\n",
    "    model_params = model_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d951e7b",
   "metadata": {
    "papermill": {
     "duration": 0.032422,
     "end_time": "2025-04-10T14:15:29.700484",
     "exception": false,
     "start_time": "2025-04-10T14:15:29.668062",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ablation 3: Forecast Horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69a9a90a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T14:15:29.766330Z",
     "iopub.status.busy": "2025-04-10T14:15:29.766016Z",
     "iopub.status.idle": "2025-04-10T14:15:29.774267Z",
     "shell.execute_reply": "2025-04-10T14:15:29.773515Z"
    },
    "papermill": {
     "duration": 0.043297,
     "end_time": "2025-04-10T14:15:29.775655",
     "exception": false,
     "start_time": "2025-04-10T14:15:29.732358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_grid_search_horizon(\n",
    "    horizon_values,\n",
    "        df_dict,\n",
    "        model_name,\n",
    "        img_tensor,\n",
    "    model_save_dir: str,\n",
    "    result_json_path: str,\n",
    "    df_params: dict = None,\n",
    "    model_params: dict = None,\n",
    "):    \n",
    "    model_dict = {}\n",
    "    results_list = []\n",
    "    \n",
    "    for idx, h in enumerate(horizon_values):\n",
    "        print(f\"Training model on dataset {idx} with horizon: {h}\")\n",
    "\n",
    "        model = get_model(img_tensor=img_tensor, forecast_horizon=h, **model_params)\n",
    "\n",
    "        ds_dict = {\n",
    "            k : get_dl(df = v, forecast_horizon = h, **df_params)\n",
    "            for k, v in df_dict.items()\n",
    "        }\n",
    "        \n",
    "        criterion = MaskedMSELoss().to(device)\n",
    "        optimizer = get_optimizer(model, lr = CFG.lr)\n",
    "        scheduler = get_reduce_lr(optimizer, factor=CFG.factor, patience=CFG.patience)\n",
    "        \n",
    "        loss_list, val_loss_list, all_loss_list = train(\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            num_epochs=CFG.num_epochs,\n",
    "            train_dataloader=ds_dict['train_dl'],\n",
    "            val_dataloader=ds_dict['test_dl'],\n",
    "            scheduler=scheduler,\n",
    "            do_validate=True,\n",
    "            validate_frequency=1,\n",
    "            verbose=-2,\n",
    "            model_name=model_save_dir + f\"{model_name}_{idx}_\"\n",
    "        )\n",
    "        \n",
    "        model_path = model_save_dir + f\"final_{model_name}_{idx}.pt\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        model_dict[idx] = model\n",
    "        \n",
    "        result_metrics = {}\n",
    "        paths = ['best', '2_5', '10']\n",
    "        for p in paths:\n",
    "            result_metrics[p] = {}\n",
    "            model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            \n",
    "            for df_name, df1 in ds_dict.items():\n",
    "                result_metrics[p][df_name] = predict_eval(model, df1, criterion, verbose=False)\n",
    "\n",
    "        results_list.append({\n",
    "            \"model_index\": idx,\n",
    "            \"model_name\": model_path,\n",
    "            \"horizon\": h,\n",
    "            \"params\" : model_params,\n",
    "            \"results\": result_metrics\n",
    "        })\n",
    "\n",
    "    with open(result_json_path, \"w\") as f:\n",
    "        json.dump(results_list, f, indent=4)\n",
    "\n",
    "    return model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95a84a41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T14:15:29.843324Z",
     "iopub.status.busy": "2025-04-10T14:15:29.843009Z",
     "iopub.status.idle": "2025-04-10T14:43:04.331231Z",
     "shell.execute_reply": "2025-04-10T14:43:04.330217Z"
    },
    "papermill": {
     "duration": 1654.524305,
     "end_time": "2025-04-10T14:43:04.333051",
     "exception": false,
     "start_time": "2025-04-10T14:15:29.808746",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 0 with horizon: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [05:20<00:00,  6.41s/it]\n",
      "<ipython-input-46-706c988bf109>:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 1 with horizon: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [05:12<00:00,  6.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 2 with horizon: 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [05:13<00:00,  6.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 3 with horizon: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [05:11<00:00,  6.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 4 with horizon: 175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 50/50 [04:55<00:00,  5.90s/it]\n"
     ]
    }
   ],
   "source": [
    "horizon_values = [35, 70, 105, 140, 175]\n",
    "\n",
    "model_params = {\n",
    "    'input_size': None,\n",
    "    'num_layers': 3,\n",
    "    'hidden_size': 64,\n",
    "}\n",
    "\n",
    "df_dict = {\n",
    "    'train_dl' : np_df[:max_train_index, :, :, :],\n",
    "    'test_dl' : np_df[max_train_index:, :, :, :],\n",
    "    'full_train_dl' : full_np_df[:max_train_index, :, :, :],\n",
    "    'full_test_dl' : full_np_df[max_train_index:, :, :, :],\n",
    "}\n",
    "\n",
    "df_params = {\n",
    "    'sequence_length' : 105,\n",
    "}\n",
    "\n",
    "model_name = \"forecast_horizon_test\"\n",
    "\n",
    "model_save_dir = f\"./{model_name}_dir/\"\n",
    "result_json_path = f\"{model_save_dir}{model_name}_results.json\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "model_dict3 = run_grid_search_horizon(\n",
    "    horizon_values,\n",
    "        df_dict,\n",
    "        model_name,\n",
    "        img_tensor,\n",
    "    model_save_dir,\n",
    "    result_json_path,\n",
    "    df_params=df_params,\n",
    "    model_params = model_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "42a404d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-10T14:43:04.423285Z",
     "iopub.status.busy": "2025-04-10T14:43:04.422980Z",
     "iopub.status.idle": "2025-04-10T14:43:04.426286Z",
     "shell.execute_reply": "2025-04-10T14:43:04.425433Z"
    },
    "papermill": {
     "duration": 0.050035,
     "end_time": "2025-04-10T14:43:04.427757",
     "exception": false,
     "start_time": "2025-04-10T14:43:04.377722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# with open(\"/kaggle/working/model_size_test_dir/model_size_test_results.json\", \"r\") as f:\n",
    "#     j = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50aa182",
   "metadata": {
    "papermill": {
     "duration": 0.044256,
     "end_time": "2025-04-10T14:43:04.516867",
     "exception": false,
     "start_time": "2025-04-10T14:43:04.472611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6950669,
     "sourceId": 11154655,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 226700844,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 228118212,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 232490461,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5122.486449,
   "end_time": "2025-04-10T14:43:07.592048",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-10T13:17:45.105599",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
