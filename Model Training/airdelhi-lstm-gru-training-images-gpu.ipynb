{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "230642f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:04.873307Z",
     "iopub.status.busy": "2025-04-11T16:22:04.873018Z",
     "iopub.status.idle": "2025-04-11T16:22:18.640055Z",
     "shell.execute_reply": "2025-04-11T16:22:18.639082Z"
    },
    "papermill": {
     "duration": 13.777906,
     "end_time": "2025-04-11T16:22:18.641827",
     "exception": false,
     "start_time": "2025-04-11T16:22:04.863921",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebrae\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "import os\n",
    "\n",
    "# from polire import IDW\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from datetime import date, timedelta\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "import pickle as pkl\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn import MSELoss\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae8c1d37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:18.658494Z",
     "iopub.status.busy": "2025-04-11T16:22:18.657945Z",
     "iopub.status.idle": "2025-04-11T16:22:18.714526Z",
     "shell.execute_reply": "2025-04-11T16:22:18.713841Z"
    },
    "papermill": {
     "duration": 0.065905,
     "end_time": "2025-04-11T16:22:18.715819",
     "exception": false,
     "start_time": "2025-04-11T16:22:18.649914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    evaluation_time_gap = 1\n",
    "    convert_numpy = False\n",
    "    target_list = ['pm2_5', 'pm10']\n",
    "    features = ['timeOfDay', 'lat', 'lon', 'distance', 'bus_count', 'day_of_week', 'pm2_5', 'pm10']\n",
    "    img_cols = ['lat_i', 'lon_j']\n",
    "\n",
    "    batch_size = 128\n",
    "    num_epochs = 50\n",
    "    day_len = 35\n",
    "\n",
    "    # model_size = 'vl'\n",
    "    model_dropout = 0.2\n",
    "\n",
    "    hidden_dim = 128\n",
    "    num_layers = 6\n",
    "    \n",
    "    lr = 3e-3\n",
    "    patience=2\n",
    "    factor=0.9\n",
    "\n",
    "    model_kind=\"gru\"\n",
    "    bidirectional=False\n",
    "\n",
    "    pm2_5_thresholds = [0, 30, 60, 90, 120, 250, 2000]\n",
    "    pm10_thresholds = [0, 50, 100, 250, 350, 430, 2000]\n",
    "    aqi_category = [\"Good\", \"Satisfactory\", \"Moderate\", \"Poor\", \"Very Poor\", \"Severe\"]\n",
    "    \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b273faf",
   "metadata": {
    "papermill": {
     "duration": 0.007011,
     "end_time": "2025-04-11T16:22:18.730622",
     "exception": false,
     "start_time": "2025-04-11T16:22:18.723611",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60efcb6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:18.745781Z",
     "iopub.status.busy": "2025-04-11T16:22:18.745495Z",
     "iopub.status.idle": "2025-04-11T16:22:18.756441Z",
     "shell.execute_reply": "2025-04-11T16:22:18.755835Z"
    },
    "papermill": {
     "duration": 0.019951,
     "end_time": "2025-04-11T16:22:18.757621",
     "exception": false,
     "start_time": "2025-04-11T16:22:18.737670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SequenceImageDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, img_list, \n",
    "                 features = None, img_cols = None, sequence_length=70,\n",
    "                 forecast_horizon=35):\n",
    "        \n",
    "        features = CFG.features if features is None else features\n",
    "        img_cols = CFG.img_cols if img_cols is None else img_cols\n",
    "        self.features = features\n",
    "        self.img_cols = img_cols\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "\n",
    "        self.split_df(df)\n",
    "        \n",
    "        self.img_list = img_list\n",
    "        self.proc_img(img_list)\n",
    "\n",
    "    def split_df(self, df):\n",
    "        df = df.sort_values(['lat', 'lon', 'date_value', 'timeOfDay'])\n",
    "        lat_lon_pairs = df.groupby(['lat', 'lon'])['pm2_5'].count().reset_index()[['lat', 'lon']].values\n",
    "\n",
    "        self.sequences = []\n",
    "\n",
    "        for i in lat_lon_pairs:\n",
    "            lat_lon_values = df[(df['lat'] == i[0]) & (df['lon'] == i[1])]\n",
    "            self.sequences.append(lat_lon_values)\n",
    "\n",
    "        self.X_seq = []\n",
    "        self.y_seq = []\n",
    "        self.masked_seq = []\n",
    "        self.img_id = []\n",
    "\n",
    "        self.total_len = 0\n",
    "        \n",
    "        for s in self.sequences:\n",
    "            s = s.sort_values(['date_value', 'timeOfDay'])\n",
    "            self.X_seq.append(np.array(s[self.features].values, dtype=np.float32))\n",
    "            self.y_seq.append(np.array(s[CFG.target_list].values, dtype=np.float32))\n",
    "            \n",
    "            self.masked_seq.append(1 - np.array(s['missing'].values, dtype=np.int8))\n",
    "            # self.masked_seq.append(np.array(s['missing'].values, dtype=np.int8))\n",
    "            \n",
    "            self.img_id.append(s[self.img_cols].values)\n",
    "            \n",
    "            curr_len = len(self.y_seq[-1])\n",
    "            curr_len -= self.sequence_length  + self.forecast_horizon - CFG.day_len\n",
    "            self.total_len += curr_len // CFG.day_len\n",
    "            self.per_len = curr_len // CFG.day_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.total_len\n",
    "\n",
    "    def proc_img(self, img_list):\n",
    "        self.imgs = {}\n",
    "\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        \n",
    "        for (i, j), img_file in img_list:\n",
    "            img = Image.open(img_file).convert(\"RGB\")\n",
    "            tensor_img = transform(img)\n",
    "            self.imgs[(i, j)] = tensor_img\n",
    "\n",
    "    def get_index(self, i):\n",
    "        lat_lon = i // self.per_len\n",
    "        within_idx = i % self.per_len\n",
    "        return lat_lon, within_idx\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        lat_lon, within_idx = self.get_index(i)\n",
    "\n",
    "        within_idx *= CFG.day_len\n",
    "        start = within_idx\n",
    "        X_end = within_idx + self.sequence_length\n",
    "        y_end = X_end + self.forecast_horizon\n",
    "        \n",
    "        \n",
    "        X = torch.tensor(self.X_seq[lat_lon][start:X_end]).to(device)\n",
    "        y = torch.tensor(self.y_seq[lat_lon][X_end:y_end]).to(device)\n",
    "        masks = torch.tensor(self.masked_seq[lat_lon][X_end:y_end] # , dtype=torch.int32\n",
    "                            ).to(device)\n",
    "        masks = torch.reshape(masks, (-1, 1))\n",
    "\n",
    "        img = self.imgs[tuple(self.img_id[lat_lon][start])]\n",
    "        \n",
    "        return X, y, masks, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0494f0c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:18.772845Z",
     "iopub.status.busy": "2025-04-11T16:22:18.772580Z",
     "iopub.status.idle": "2025-04-11T16:22:20.158141Z",
     "shell.execute_reply": "2025-04-11T16:22:20.157264Z"
    },
    "papermill": {
     "duration": 1.395006,
     "end_time": "2025-04-11T16:22:20.159718",
     "exception": false,
     "start_time": "2025-04-11T16:22:18.764712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Timestamp('2020-11-01 00:00:00'), Timestamp('2021-01-07 00:00:00'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/kaggle/input/airdelhi-baselines-deepengineering/dense_df.csv')\n",
    "\n",
    "df = df.drop(columns = 'Unnamed: 0')\n",
    "df['date_value'] = pd.to_datetime(df['date_value'])\n",
    "\n",
    "dates = pd.to_datetime([df['date_value'].min(), df['date_value'].max()])\n",
    "\n",
    "max_train_date = dates.min() + (dates.max() - dates.min()) * 0.75\n",
    "max_train_date = max_train_date.floor(\"D\")\n",
    "min_train_date = df['date_value'].min()\n",
    "max_date = df['date_value'].max().floor(\"D\")\n",
    "\n",
    "metrics_dict = {\n",
    "    'MSE': mean_squared_error, \n",
    "    'r2 score': r2_score, \n",
    "    'MAE': mean_absolute_error,\n",
    "}\n",
    "\n",
    "target_list = CFG.target_list\n",
    "\n",
    "features = ['date_value', 'timeOfDay', 'lat', 'lon', 'day_of_week', 'distance', 'bus_count']\n",
    "\n",
    "CFG.base_features = ['timeOfDay', 'lat', 'lon', 'day_of_week', 'distance', 'bus_count']\n",
    "CFG.features = CFG.base_features\n",
    "\n",
    "min_train_date, max_train_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09e34e7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:20.175818Z",
     "iopub.status.busy": "2025-04-11T16:22:20.175522Z",
     "iopub.status.idle": "2025-04-11T16:22:23.430543Z",
     "shell.execute_reply": "2025-04-11T16:22:23.429595Z"
    },
    "papermill": {
     "duration": 3.264743,
     "end_time": "2025-04-11T16:22:23.432151",
     "exception": false,
     "start_time": "2025-04-11T16:22:20.167408",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a10b91631712>:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = grouped.apply(fill_na_with_previous_mean)\n"
     ]
    }
   ],
   "source": [
    "def add_lag_features(df, lags = [1]):\n",
    "\n",
    "    df = df.copy()\n",
    "    \n",
    "    df.sort_values(by=[\"lat\", \"lon\", \"timeOfDay\", \"date_value\"], inplace=True)\n",
    "\n",
    "    added_features = []\n",
    "    for l in lags:\n",
    "        df[f'pm2_5_lag_{l}'] = df.groupby(\n",
    "            ['timeOfDay', 'lat', 'lon'])['pm2_5'].shift(l)\n",
    "        df[f'pm10_lag_{l}'] = df.groupby(\n",
    "            ['timeOfDay', 'lat', 'lon'])['pm10'].shift(l)\n",
    "\n",
    "        added_features.append(f'pm2_5_lag_{l}')\n",
    "        added_features.append(f'pm10_lag_{l}')\n",
    "\n",
    "        df.sort_values(by=[\"lat\", \"lon\", \"date_value\"], inplace=True)\n",
    "\n",
    "    # Group by latitude and longitude\n",
    "    grouped = df.groupby([\"lat\", \"lon\"])\n",
    "\n",
    "    # Function to fill NaN values based on previous mean\n",
    "    def fill_na_with_previous_mean(group):\n",
    "        for col in group.columns:\n",
    "            if col not in [\"date_value\", \"lat\", \"lon\"]:\n",
    "                group[col] = group[col].astype(float)  # Ensure numeric columns\n",
    "                group[col] = group[col].fillna(group[col].expanding().mean().shift())  # Previous days' mean\n",
    "                \n",
    "                # If still NaN (first row), replace with overall mean\n",
    "                overall_mean = df[col].mean(skipna=True)\n",
    "                group[col] = group[col].fillna(overall_mean)\n",
    "        return group\n",
    "\n",
    "    # Apply the function to each group\n",
    "    df = grouped.apply(fill_na_with_previous_mean)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df = df.sort_values(by = ['date_value', 'timeOfDay', 'lat', 'lon'])\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    CFG.features += added_features\n",
    "    return df\n",
    "    \n",
    "df = add_lag_features(df, lags = [7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c716d13e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:23.449158Z",
     "iopub.status.busy": "2025-04-11T16:22:23.448913Z",
     "iopub.status.idle": "2025-04-11T16:22:23.532090Z",
     "shell.execute_reply": "2025-04-11T16:22:23.531391Z"
    },
    "papermill": {
     "duration": 0.092741,
     "end_time": "2025-04-11T16:22:23.533602",
     "exception": false,
     "start_time": "2025-04-11T16:22:23.440861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['lat', 'lon', 'timeOfDay', 'date_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02675ff9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:23.549829Z",
     "iopub.status.busy": "2025-04-11T16:22:23.549553Z",
     "iopub.status.idle": "2025-04-11T16:22:23.661060Z",
     "shell.execute_reply": "2025-04-11T16:22:23.660084Z"
    },
    "papermill": {
     "duration": 0.121386,
     "end_time": "2025-04-11T16:22:23.662806",
     "exception": false,
     "start_time": "2025-04-11T16:22:23.541420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomScaler:\n",
    "\n",
    "    def __init__(self, df):\n",
    "        self.pm2_5_max = float(df['pm2_5'].max())\n",
    "        self.pm2_5_min = float(df['pm2_5'].min())\n",
    "        \n",
    "        self.pm10_max = float(df['pm10'].max())\n",
    "        self.pm10_min = float(df['pm10'].min())\n",
    "\n",
    "    def transform(self, df):\n",
    "        df = df.copy()\n",
    "        df['pm2_5'] = (df['pm2_5'] - self.pm2_5_min) / (self.pm2_5_max - self.pm2_5_min)\n",
    "        df['pm10'] = (df['pm10'] - self.pm10_min) / (self.pm10_max - self.pm10_min)\n",
    "        return df\n",
    "\n",
    "    def inverse_transform(self, X):\n",
    "        X[:, 0] = X[:, 0] * (self.pm2_5_max - self.pm2_5_min) + self.pm2_5_min\n",
    "        X[:, 1] = X[:, 1] * (self.pm10_max - self.pm10_min) + self.pm10_min\n",
    "        return X\n",
    "\n",
    "target_scaler = CustomScaler(df)\n",
    "df = target_scaler.transform(df)\n",
    "\n",
    "CFG.features = ['timeOfDay', 'lat', 'lon', 'distance', 'bus_count', \n",
    "                'day_of_week', 'pm2_5', 'pm10', 'pm2_5_lag_7', 'pm10_lag_7']\n",
    "\n",
    "df['day_of_week'] = df['date_value'].dt.dayofweek\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df[CFG.features] = scaler.fit_transform(df[CFG.features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2affede2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:23.680008Z",
     "iopub.status.busy": "2025-04-11T16:22:23.679729Z",
     "iopub.status.idle": "2025-04-11T16:22:23.691957Z",
     "shell.execute_reply": "2025-04-11T16:22:23.691316Z"
    },
    "papermill": {
     "duration": 0.021824,
     "end_time": "2025-04-11T16:22:23.693150",
     "exception": false,
     "start_time": "2025-04-11T16:22:23.671326",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"/kaggle/input/delhi-satellite-images-grid/tile_metadata.json\", 'r') as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6f56e17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:23.708238Z",
     "iopub.status.busy": "2025-04-11T16:22:23.708025Z",
     "iopub.status.idle": "2025-04-11T16:22:23.712441Z",
     "shell.execute_reply": "2025-04-11T16:22:23.711811Z"
    },
    "papermill": {
     "duration": 0.013242,
     "end_time": "2025-04-11T16:22:23.713690",
     "exception": false,
     "start_time": "2025-04-11T16:22:23.700448",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_metadata(metadata):\n",
    "    img_list = []\n",
    "    for k, v in metadata.items():\n",
    "        i = v['row']\n",
    "        j = v['col']\n",
    "        img_list.append([(i, j), \n",
    "            f\"/kaggle/input/delhi-satellite-images-grid/tiles_32x32/tile_{i}_{j}.png\"])\n",
    "\n",
    "    return img_list\n",
    "\n",
    "img_list = process_metadata(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea8ea234",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:23.728666Z",
     "iopub.status.busy": "2025-04-11T16:22:23.728453Z",
     "iopub.status.idle": "2025-04-11T16:22:23.742307Z",
     "shell.execute_reply": "2025-04-11T16:22:23.741620Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.02254,
     "end_time": "2025-04-11T16:22:23.743522",
     "exception": false,
     "start_time": "2025-04-11T16:22:23.720982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04999999999997419\n",
      "0.05263157894735837\n"
     ]
    }
   ],
   "source": [
    "lat = df['lat'].unique()\n",
    "lon = df['lon'].unique()\n",
    "\n",
    "lat = np.sort(lat)\n",
    "lon = np.sort(lon)\n",
    "\n",
    "min_dist_lat = 1000\n",
    "for i in range(len(lat) - 1):\n",
    "    min_dist_lat = min(min_dist_lat, lat[i+1] - lat[i])\n",
    "print(min_dist_lat)\n",
    "\n",
    "for i in range(len(lat)):\n",
    "    assert (lat[i] / min_dist_lat - round(lat[i] / min_dist_lat)) < 1e-6\n",
    "\n",
    "min_dist_lon = 1000\n",
    "for i in range(len(lon) - 1):\n",
    "    min_dist_lon = min(min_dist_lon, lon[i+1] - lon[i])\n",
    "print(min_dist_lon)\n",
    "\n",
    "for i in range(len(lon)):\n",
    "    assert (lon[i] / min_dist_lon - round(lon[i] / min_dist_lon)) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4610da1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:23.758477Z",
     "iopub.status.busy": "2025-04-11T16:22:23.758274Z",
     "iopub.status.idle": "2025-04-11T16:22:23.793841Z",
     "shell.execute_reply": "2025-04-11T16:22:23.793093Z"
    },
    "papermill": {
     "duration": 0.044865,
     "end_time": "2025-04-11T16:22:23.795675",
     "exception": false,
     "start_time": "2025-04-11T16:22:23.750810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rounded_lat_lon(df):\n",
    "    df = df.copy()\n",
    "    df['lat_i'] = np.round(df['lat'] / min_dist_lat) + 2\n",
    "    df['lon_j'] = np.round(df['lon'] / min_dist_lon) + 4\n",
    "    return df\n",
    "\n",
    "df = rounded_lat_lon(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d6ec999",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:23.811880Z",
     "iopub.status.busy": "2025-04-11T16:22:23.811616Z",
     "iopub.status.idle": "2025-04-11T16:22:31.494421Z",
     "shell.execute_reply": "2025-04-11T16:22:31.493676Z"
    },
    "papermill": {
     "duration": 7.692346,
     "end_time": "2025-04-11T16:22:31.496026",
     "exception": false,
     "start_time": "2025-04-11T16:22:23.803680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_ds = SequenceImageDataset(df[df['date_value'] <= max_train_date], img_list)\n",
    "test_ds = SequenceImageDataset(df[df['date_value'] > max_train_date], img_list)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size = CFG.batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size = CFG.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3960b501",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:31.512131Z",
     "iopub.status.busy": "2025-04-11T16:22:31.511891Z",
     "iopub.status.idle": "2025-04-11T16:22:31.695487Z",
     "shell.execute_reply": "2025-04-11T16:22:31.694653Z"
    },
    "papermill": {
     "duration": 0.192936,
     "end_time": "2025-04-11T16:22:31.696847",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.503911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([70, 10]),\n",
       " torch.Size([35, 2]),\n",
       " torch.Size([35, 1]),\n",
       " torch.Size([3, 32, 32]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt, yt, mt, i = train_ds[0]\n",
    "xt, yt, mt, i2 = train_ds[1]\n",
    "xt.shape, yt.shape, mt.shape, i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8ecbbc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:31.712824Z",
     "iopub.status.busy": "2025-04-11T16:22:31.712549Z",
     "iopub.status.idle": "2025-04-11T16:22:31.718392Z",
     "shell.execute_reply": "2025-04-11T16:22:31.717577Z"
    },
    "papermill": {
     "duration": 0.014959,
     "end_time": "2025-04-11T16:22:31.719600",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.704641",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert torch.all(i == i2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bded8f1d",
   "metadata": {
    "papermill": {
     "duration": 0.007042,
     "end_time": "2025-04-11T16:22:31.733992",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.726950",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## RNN / GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8f2371b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:31.749472Z",
     "iopub.status.busy": "2025-04-11T16:22:31.749226Z",
     "iopub.status.idle": "2025-04-11T16:22:31.760610Z",
     "shell.execute_reply": "2025-04-11T16:22:31.759811Z"
    },
    "papermill": {
     "duration": 0.020679,
     "end_time": "2025-04-11T16:22:31.761975",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.741296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageMultiStepPredictor(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size, \n",
    "        hidden_size, \n",
    "        num_layers, \n",
    "        dropout=0.2,\n",
    "        output_size=2, \n",
    "        forecast_horizon=35,\n",
    "        model_kind=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        RNN-based model for multi-step time-series forecasting.\n",
    "\n",
    "        Also takes in a satellite image representing the area being forecasted over\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Number of input features.\n",
    "            hidden_size (int): Hidden state size in RNN.\n",
    "            num_layers (int): Number of stacked RNN layers.\n",
    "            output_size (int): Number of target variables.\n",
    "            forecast_horizon (int): Number of future time-steps to predict.\n",
    "        \"\"\"\n",
    "        super(ImageMultiStepPredictor, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        if model_kind is None:\n",
    "            model_kind = CFG.model_kind\n",
    "        \n",
    "        if model_kind == 'rnn':\n",
    "            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        elif model_kind == 'lstm':\n",
    "            self.rnn = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                               batch_first=True, dropout=dropout,\n",
    "                               bidirectional = CFG.bidirectional)\n",
    "        elif model_kind == 'gru':\n",
    "            self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                              batch_first=True, dropout=dropout,\n",
    "                              bidirectional = CFG.bidirectional)\n",
    "        \n",
    "        self.rnn = self.rnn.to(device)\n",
    "        # self.conv_model = nn.Sequential(\n",
    "        #     nn.Conv2d(3, 32, 3), # 30x30\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2), # 15x15\n",
    "        #     nn.Conv2d(32, 64, 4), # 12x12\n",
    "        #     nn.BatchNorm2d(64),\n",
    "        #     nn.Dropout(dropout),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2), # 6x6\n",
    "        #     nn.Conv2d(64, 64, 3), # 4x4\n",
    "        #     nn.BatchNorm2d(64),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(dropout),\n",
    "        #     nn.MaxPool2d(2), # 2x2\n",
    "        #     nn.Flatten(),\n",
    "        #     nn.Linear(4*64, 32),\n",
    "        #     nn.Dropout(dropout),\n",
    "        #     nn.ReLU()\n",
    "        # )\n",
    "\n",
    "        self.conv_model = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, 3), # 30x30\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 15x15\n",
    "            nn.Conv2d(16, 32, 4), # 12x12\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2), # 6x6\n",
    "            nn.Conv2d(32, 32, 3), # 4x4\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.MaxPool2d(2), # 2x2\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(4*32, 32),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLU()\n",
    "        ).to(device)\n",
    "        \n",
    "        # Fully connected layer maps hidden state to output\n",
    "        self.fc = nn.Linear(hidden_size + 32, forecast_horizon * output_size).to(device)\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.dim() < 2:\n",
    "                nn.init.zeros_(param)\n",
    "            elif 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param)\n",
    "            elif 'fc' in name and 'weight' in name:\n",
    "                nn.init.kaiming_uniform_(param, nonlinearity='relu')\n",
    "            elif 'conv' in name and 'weight' in name:\n",
    "                nn.init.kaiming_normal_(param, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "\n",
    "    def forward(self, x, img):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "        out, _ = self.rnn(x, h0)  # (batch_size, sequence_length, hidden_size)\n",
    "        last_hidden_state = out[:, -1, :]  # (batch_size, hidden_size)\n",
    "\n",
    "        features = self.conv_model(img)\n",
    "\n",
    "        h = torch.cat([last_hidden_state, features], dim=1)\n",
    "\n",
    "        out = self.fc(h)  # (batch_size, forecast_horizon * output_size)\n",
    "\n",
    "        # Reshape to (batch_size, forecast_horizon, output_size)\n",
    "        out = out.view(batch_size, self.forecast_horizon, -1)\n",
    "        return out\n",
    "\n",
    "def get_optimizer(model, lr = 1e-4, weight_decay = 1e-5):\n",
    "    optimizer = Adam(params = model.parameters(), \n",
    "                     lr = lr, weight_decay = weight_decay)\n",
    "    return optimizer\n",
    "\n",
    "def get_reduce_lr(optimizer, factor=0.1, patience=2):\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode = 'min', factor = factor, patience = patience)\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cddcf3f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:31.777325Z",
     "iopub.status.busy": "2025-04-11T16:22:31.777090Z",
     "iopub.status.idle": "2025-04-11T16:22:31.780878Z",
     "shell.execute_reply": "2025-04-11T16:22:31.780088Z"
    },
    "papermill": {
     "duration": 0.012672,
     "end_time": "2025-04-11T16:22:31.782047",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.769375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(input_size = 10, hidden_size = 32, num_layers = 3, forecast_horizon=35,\n",
    "             model_kind = 'gru'):\n",
    "    if input_size is None:\n",
    "        input_size = len(CFG.features)\n",
    "    model = ImageMultiStepPredictor(input_size, hidden_size, num_layers, \n",
    "                               output_size=2, forecast_horizon=forecast_horizon,\n",
    "                                   dropout=CFG.model_dropout, model_kind=model_kind)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba769a",
   "metadata": {
    "papermill": {
     "duration": 0.007042,
     "end_time": "2025-04-11T16:22:31.796321",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.789279",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "982e0c7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:31.811604Z",
     "iopub.status.busy": "2025-04-11T16:22:31.811338Z",
     "iopub.status.idle": "2025-04-11T16:22:31.818473Z",
     "shell.execute_reply": "2025-04-11T16:22:31.817663Z"
    },
    "papermill": {
     "duration": 0.016307,
     "end_time": "2025-04-11T16:22:31.819820",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.803513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MaskedMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, preds, targets, mask):\n",
    "        loss = (preds - targets) ** 2\n",
    "        masked_loss = (loss * mask).sum() / (mask.sum() + self.eps)\n",
    "        return masked_loss\n",
    "\n",
    "class MaskedMAELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(MaskedMAELoss, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, preds, targets, mask):\n",
    "        loss = abs(preds - targets)\n",
    "        masked_loss = (loss * mask).sum() / (mask.sum() + self.eps)\n",
    "        return masked_loss\n",
    "\n",
    "class MaskedR2(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super(MaskedR2, self).__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, preds, targets, mask):\n",
    "        error = ((targets - preds) ** 2) * mask\n",
    "        masked_mse = error.sum() / (mask.sum() + self.eps)\n",
    "        mean_y = (targets * mask).sum() / (mask.sum() + self.eps)\n",
    "        total_var = (((targets - mean_y) ** 2) * mask).sum() / (mask.sum() + self.eps)\n",
    "        r2 = 1 - (masked_mse / (total_var + self.eps))\n",
    "        return r2\n",
    "\n",
    "masked_metrics_dict = {\n",
    "    'MSE': MaskedMSELoss(),\n",
    "    'MAE': MaskedMAELoss(),\n",
    "    'r2 score': MaskedR2(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d5f90d23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:31.835593Z",
     "iopub.status.busy": "2025-04-11T16:22:31.835362Z",
     "iopub.status.idle": "2025-04-11T16:22:31.840697Z",
     "shell.execute_reply": "2025-04-11T16:22:31.839953Z"
    },
    "papermill": {
     "duration": 0.014359,
     "end_time": "2025-04-11T16:22:31.842011",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.827652",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RunningLoss:\n",
    "    def __init__(self, window = 10):\n",
    "        self.loss = 0\n",
    "        self.total = 0\n",
    "        self.loss_last = []\n",
    "        self.total_last = []\n",
    "\n",
    "        self.window = window\n",
    "    \n",
    "    def update(self, loss, batch_size):\n",
    "        total = 1\n",
    "        self.loss += loss\n",
    "        self.total += 1\n",
    "        \n",
    "        self.loss_last.append(loss)\n",
    "        self.total_last.append(total)\n",
    "        if len(self.loss_last) > self.window:\n",
    "            self.loss_last.pop(0)\n",
    "            self.total_last.pop(0)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.loss = 0\n",
    "        self.total = 0\n",
    "        self.loss_last = []\n",
    "        self.total_last = []\n",
    "\n",
    "    def print(self):\n",
    "        print(f\"Accuracy: {self.loss / self.total}\")\n",
    "    \n",
    "    def get_curr_stats(self):\n",
    "        return sum(self.loss_last) / sum(self.total_last)\n",
    "\n",
    "    def total_stats(self):\n",
    "        return self.loss / self.total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e94a145",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:31.857316Z",
     "iopub.status.busy": "2025-04-11T16:22:31.857115Z",
     "iopub.status.idle": "2025-04-11T16:22:31.863291Z",
     "shell.execute_reply": "2025-04-11T16:22:31.862693Z"
    },
    "papermill": {
     "duration": 0.015061,
     "end_time": "2025-04-11T16:22:31.864413",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.849352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate(\n",
    "    model,\n",
    "    test_dl,\n",
    "    criterion,\n",
    "    verbose=True,\n",
    "    get_predict=False,\n",
    "):\n",
    "    model.eval()\n",
    "    running_loss = RunningLoss()\n",
    "\n",
    "    predict_array = []\n",
    "    true_array = []\n",
    "    mask_array = []\n",
    "    \n",
    "    if verbose >= 1:\n",
    "        pbar = tqdm(total = len(test_dl), ncols = 110, desc = \"Validation Progress\") # , dynamic_ncols=True, leave=False)\n",
    "    else:\n",
    "        pbar = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y, mask, img) in enumerate(test_dl):\n",
    "            x, y, mask, img = x.to(device), y.to(device), mask.to(device), img.to(device)\n",
    "            pred = model(x, img)\n",
    "\n",
    "            pred = target_scaler.inverse_transform(pred)\n",
    "            y = target_scaler.inverse_transform(y)\n",
    "\n",
    "            temp_loss = criterion(pred, y, mask)\n",
    "            running_loss.update(temp_loss.item(), batch_size=x.shape[0])\n",
    "\n",
    "            if get_predict:\n",
    "                predict_array.append(pred)\n",
    "                true_array.append(y)\n",
    "                mask_array.append(mask)\n",
    "\n",
    "            if verbose >= 1:\n",
    "                total = len(test_dl)\n",
    "                curr_loss = np.sqrt(running_loss.get_curr_stats())\n",
    "                pbar.set_description(f\"Validation Step {i} / {total}\")\n",
    "                pbar.set_postfix(Loss=f\"{curr_loss:.4f}\")\n",
    "                pbar.update(1)\n",
    "        if verbose >= 1:\n",
    "            pbar.close()\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    if get_predict:\n",
    "        return predict_array, true_array, mask_array, np.sqrt(running_loss.total_stats())\n",
    "    \n",
    "    return np.sqrt(running_loss.total_stats())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e6993c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:31.879789Z",
     "iopub.status.busy": "2025-04-11T16:22:31.879569Z",
     "iopub.status.idle": "2025-04-11T16:22:31.891514Z",
     "shell.execute_reply": "2025-04-11T16:22:31.890964Z"
    },
    "papermill": {
     "duration": 0.021006,
     "end_time": "2025-04-11T16:22:31.892698",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.871692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        num_epochs,\n",
    "        train_dataloader,\n",
    "        val_dataloader,\n",
    "        scheduler=None,\n",
    "        do_validate=False,\n",
    "        validate_frequency=1,\n",
    "        verbose=True,\n",
    "        metrics_dict=None,\n",
    "        model_name = \"\"\n",
    "):\n",
    "    if metrics_dict is None:\n",
    "        metrics_dict = masked_metrics_dict\n",
    "    \n",
    "    loss_list = []\n",
    "    val_loss_list = []\n",
    "    all_loss_list = []\n",
    "\n",
    "    best_val_loss = 100000000\n",
    "    best_val_loss25 = 100000000\n",
    "    best_val_loss10 = 100000000\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs), disable=verbose >= -1):\n",
    "        running_loss = RunningLoss()\n",
    "        if verbose >= 1:\n",
    "            pbar = tqdm(total = len(train_dataloader), ncols = 110, desc = \"Training Progress\") # , dynamic_ncols=True, leave=False)\n",
    "        else:\n",
    "            pbar = None\n",
    "        for i, (x, y, mask, img) in enumerate(train_dataloader):\n",
    "            x, y, mask, img = x.to(device), y.to(device), mask.to(device), img.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            pred = model(x, img)\n",
    "            loss = criterion(pred, y, mask)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            all_loss_list.append(loss.item())\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = target_scaler.inverse_transform(pred)\n",
    "                y = target_scaler.inverse_transform(y)\n",
    "                temp_loss = torch.sqrt(criterion(pred, y, mask))\n",
    "                running_loss.update(temp_loss.item(), batch_size=x.shape[0])\n",
    "\n",
    "            if verbose >= 1:\n",
    "                total = len(train_dataloader)\n",
    "                curr_loss = running_loss.get_curr_stats()\n",
    "                pbar.set_description(f\"Epoch {epoch}: Step {i} / {total}\")\n",
    "                pbar.set_postfix(Loss=f\"{curr_loss:.4f}\")\n",
    "                pbar.update(1)\n",
    "        if verbose >= 1:\n",
    "            print(f\"Train Loss Total: {running_loss.total_stats()}\")\n",
    "            pbar.close()\n",
    "        \n",
    "        if do_validate and epoch % validate_frequency == validate_frequency - 1:\n",
    "            pred1, y1, mask1, val_loss = validate(model, val_dataloader, criterion, \n",
    "                                           verbose=verbose, get_predict=True)\n",
    "            pred1 = torch.concat(pred1)\n",
    "            y1 = torch.concat(y1)\n",
    "            mask1 = torch.concat(mask1)\n",
    "            \n",
    "            val_loss_list.append(val_loss)\n",
    "            if scheduler is not None:\n",
    "                scheduler.step(val_loss)\n",
    "            \n",
    "            if verbose >= 1:\n",
    "                print(f\"Val Loss Total: {val_loss}\")\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                if verbose >= 0:\n",
    "                    print(f\"Better Val Loss: {val_loss} < {best_val_loss}\")\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model.state_dict(), f\"./{model_name}best.pth\")\n",
    "            \n",
    "            for i, t in enumerate(CFG.target_list):\n",
    "                for n, m in metrics_dict.items():\n",
    "                    if n == 'MSE':\n",
    "                        # print(y1[:, :, i].shape, pred1[:, :, i].shape, mask1.shape)\n",
    "                        mse_loss = np.sqrt(m(pred1[:, :, i], y1[:, :, i], mask1[:, :, 0]).cpu().numpy())\n",
    "                        if verbose >= 1:\n",
    "                            print(f\"{t}: {n}: {mse_loss}\")\n",
    "                        \n",
    "                        if i == 0 and mse_loss < best_val_loss25:\n",
    "                            if verbose >= 0:\n",
    "                                print(f\"----------Better MSE on pm2_5 {mse_loss} < {best_val_loss25}\")\n",
    "                            torch.save(model.state_dict(), f\"./{model_name}2_5.pth\")\n",
    "                            best_val_loss25 = mse_loss\n",
    "                        if i == 1 and mse_loss < best_val_loss10:\n",
    "                            if verbose >= 0:\n",
    "                                print(f\"----------Better MSE on pm10 {mse_loss} < {best_val_loss10}\")\n",
    "                            torch.save(model.state_dict(), f\"./{model_name}10.pth\")\n",
    "                            best_val_loss10 = mse_loss\n",
    "                    \n",
    "                    if verbose >= 1:\n",
    "                        if n == 'MSE':\n",
    "                            mse_loss = np.sqrt(m(pred1[:, :, i], y1[:, :, i], mask1[:, :, 0]).cpu().numpy())\n",
    "                            print(f\"{t}: {n}: {mse_loss}\")\n",
    "                        else:\n",
    "                            print(f\"{t}: {n}: {(m(pred1[:, :, i], y1[:, :, i], mask1[:, :, 0]))}\")\n",
    "        \n",
    "        epoch_loss = running_loss.total_stats()\n",
    "        loss_list.append(epoch_loss)\n",
    "    \n",
    "    return loss_list, val_loss_list, all_loss_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff693f8",
   "metadata": {
    "papermill": {
     "duration": 0.007037,
     "end_time": "2025-04-11T16:22:31.907155",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.900118",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3f65ea79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:31.922567Z",
     "iopub.status.busy": "2025-04-11T16:22:31.922265Z",
     "iopub.status.idle": "2025-04-11T16:22:31.925532Z",
     "shell.execute_reply": "2025-04-11T16:22:31.924664Z"
    },
    "papermill": {
     "duration": 0.012698,
     "end_time": "2025-04-11T16:22:31.927021",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.914323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = get_model(hidden_size = CFG.hidden_dim, num_layers = CFG.num_layers).to(device)\n",
    "# criterion = MaskedMSELoss().to(device)\n",
    "# optimizer = get_optimizer(model, lr = CFG.lr)\n",
    "# scheduler = get_reduce_lr(optimizer, factor=CFG.factor, patience=CFG.patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60af3ba4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:31.942253Z",
     "iopub.status.busy": "2025-04-11T16:22:31.942042Z",
     "iopub.status.idle": "2025-04-11T16:22:31.944922Z",
     "shell.execute_reply": "2025-04-11T16:22:31.944270Z"
    },
    "papermill": {
     "duration": 0.011881,
     "end_time": "2025-04-11T16:22:31.946050",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.934169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# validate(model, test_dl, criterion, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2d37d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:31.961411Z",
     "iopub.status.busy": "2025-04-11T16:22:31.961166Z",
     "iopub.status.idle": "2025-04-11T16:22:31.964226Z",
     "shell.execute_reply": "2025-04-11T16:22:31.963431Z"
    },
    "papermill": {
     "duration": 0.012155,
     "end_time": "2025-04-11T16:22:31.965560",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.953405",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # loss_list, val_loss_list, all_loss_list\n",
    "# # pred1, y1, \n",
    "# loss_list, val_loss_list, all_loss_list = train(\n",
    "#     model,\n",
    "#     criterion,\n",
    "#     optimizer,\n",
    "#     num_epochs=CFG.num_epochs,\n",
    "#     train_dataloader=train_dl,\n",
    "#     val_dataloader=test_dl,\n",
    "#     scheduler=scheduler,\n",
    "#     do_validate=True,\n",
    "#     verbose=-2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39fc16b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:31.980988Z",
     "iopub.status.busy": "2025-04-11T16:22:31.980717Z",
     "iopub.status.idle": "2025-04-11T16:22:31.987608Z",
     "shell.execute_reply": "2025-04-11T16:22:31.986821Z"
    },
    "papermill": {
     "duration": 0.015976,
     "end_time": "2025-04-11T16:22:31.988955",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.972979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_aqi_category_indices(preds, thresholds):\n",
    "    indices = torch.bucketize(preds, torch.tensor(thresholds).to(device), right=False) - 1\n",
    "    return torch.clamp(indices, min=0, max=5)\n",
    "\n",
    "def compute_aqi_classification_metrics(pred, label, mask):\n",
    "    \"\"\"\n",
    "    Compute per-class accuracy, precision, and recall separately for PM2.5 and PM10.\n",
    "\n",
    "    Args:\n",
    "        pred: Tensor of predicted AQI values of shape [N, S, 2].\n",
    "        label: Tensor of ground truth AQI values of shape [N, S, 2].\n",
    "        mask: Tensor of 0s and 1s indicating valid positions of shape [N, S, 1].\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with separate per-class metrics for PM2.5 and PM10.\n",
    "    \"\"\"\n",
    "    if mask.shape[-1] == 1:\n",
    "        mask = mask.expand(-1, -1, 2)\n",
    "    mask = mask.bool()\n",
    "\n",
    "    metrics = {}\n",
    "    for i, (name, thresholds) in enumerate(zip(CFG.target_list, [CFG.pm2_5_thresholds, CFG.pm10_thresholds])):\n",
    "        pred_class = get_aqi_category_indices(pred[..., i], thresholds)\n",
    "        label_class = get_aqi_category_indices(label[..., i], thresholds)\n",
    "\n",
    "        pred_class = pred_class[mask[..., i]]\n",
    "        label_class = label_class[mask[..., i]]\n",
    "\n",
    "        class_metrics = {}\n",
    "        for class_idx, class_name in enumerate(CFG.aqi_category):\n",
    "            true_pos = ((pred_class == class_idx) & (label_class == class_idx)).sum().item()\n",
    "            total_pred = (pred_class == class_idx).sum().item()\n",
    "            total_true = (label_class == class_idx).sum().item()\n",
    "\n",
    "            accuracy = true_pos / total_true if total_true else 0.0\n",
    "            precision = true_pos / total_pred if total_pred else 0.0\n",
    "            recall = true_pos / total_true if total_true else 0.0\n",
    "\n",
    "            class_metrics[class_name] = {\n",
    "                \"true\": true_pos,\n",
    "                \"total\": total_true,\n",
    "                \"accuracy\": accuracy,\n",
    "                \"precision\": precision,\n",
    "                \"recall\": recall\n",
    "            }\n",
    "\n",
    "        metrics[name] = class_metrics\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c34e28b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:32.004145Z",
     "iopub.status.busy": "2025-04-11T16:22:32.003937Z",
     "iopub.status.idle": "2025-04-11T16:22:32.009098Z",
     "shell.execute_reply": "2025-04-11T16:22:32.008456Z"
    },
    "papermill": {
     "duration": 0.014111,
     "end_time": "2025-04-11T16:22:32.010272",
     "exception": false,
     "start_time": "2025-04-11T16:22:31.996161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_eval(model, test_dl, criterion, verbose=True):\n",
    "    o = validate(model, test_dl, criterion, verbose=verbose, get_predict=True)\n",
    "    pred = torch.concat(o[0])\n",
    "    y = torch.concat(o[1])\n",
    "    mask = torch.concat(o[2])\n",
    "    model_performance = {0 : {}, 1 : {}}\n",
    "    for i, t in enumerate(CFG.target_list):\n",
    "        for n, m in masked_metrics_dict.items():\n",
    "            l = m(pred[:, :, i], y[:, :, i], mask[:, :, 0])\n",
    "            if n == 'MSE':\n",
    "                # print(y1[:, :, i].shape, pred1[:, :, i].shape, mask1.shape)\n",
    "                l = np.sqrt(l.cpu().numpy())\n",
    "                if verbose:\n",
    "                    print(f\"{t}: {n}: {l}\")\n",
    "            elif verbose:\n",
    "                print(f\"{t}: {n}: {l}\")\n",
    "\n",
    "            model_performance[i][n] = float(l)\n",
    "\n",
    "    model_performance['classification'] = compute_aqi_classification_metrics(pred, y, mask)\n",
    "    \n",
    "    return model_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "daa580d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:32.026027Z",
     "iopub.status.busy": "2025-04-11T16:22:32.025802Z",
     "iopub.status.idle": "2025-04-11T16:22:32.028371Z",
     "shell.execute_reply": "2025-04-11T16:22:32.027796Z"
    },
    "papermill": {
     "duration": 0.011749,
     "end_time": "2025-04-11T16:22:32.029547",
     "exception": false,
     "start_time": "2025-04-11T16:22:32.017798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_eval(model, test_dl, criterion, verbose=True)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6a124b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:32.044845Z",
     "iopub.status.busy": "2025-04-11T16:22:32.044578Z",
     "iopub.status.idle": "2025-04-11T16:22:32.047417Z",
     "shell.execute_reply": "2025-04-11T16:22:32.046797Z"
    },
    "papermill": {
     "duration": 0.011718,
     "end_time": "2025-04-11T16:22:32.048625",
     "exception": false,
     "start_time": "2025-04-11T16:22:32.036907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# predict_eval(model, test_dl, criterion, verbose=True)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d66594",
   "metadata": {
    "papermill": {
     "duration": 0.007682,
     "end_time": "2025-04-11T16:22:32.063562",
     "exception": false,
     "start_time": "2025-04-11T16:22:32.055880",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Df 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60ee7700",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:32.078840Z",
     "iopub.status.busy": "2025-04-11T16:22:32.078539Z",
     "iopub.status.idle": "2025-04-11T16:22:48.634644Z",
     "shell.execute_reply": "2025-04-11T16:22:48.633971Z"
    },
    "papermill": {
     "duration": 16.565459,
     "end_time": "2025-04-11T16:22:48.636180",
     "exception": false,
     "start_time": "2025-04-11T16:22:32.070721",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-a10b91631712>:35: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = grouped.apply(fill_na_with_previous_mean)\n"
     ]
    }
   ],
   "source": [
    "full_df = pd.read_csv('/kaggle/input/airdelhi-baselines-deepengineering/full_dense_df.csv')\n",
    "\n",
    "full_df = full_df.drop(columns = 'Unnamed: 0')\n",
    "full_df['date_value'] = pd.to_datetime(full_df['date_value'])\n",
    "\n",
    "full_df = add_lag_features(full_df, lags = [7])\n",
    "CFG.features = ['timeOfDay', 'lat', 'lon', 'distance', 'bus_count', \n",
    "                'day_of_week', 'pm2_5', 'pm10', 'pm2_5_lag_7', 'pm10_lag_7']\n",
    "full_df = full_df.sort_values(by=['lat', 'lon', 'timeOfDay', 'date_value'])\n",
    "\n",
    "full_df = target_scaler.transform(full_df)\n",
    "\n",
    "full_df['day_of_week'] = full_df['date_value'].dt.dayofweek\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "full_df[CFG.features] = scaler.fit_transform(full_df[CFG.features])\n",
    "\n",
    "full_df = rounded_lat_lon(full_df)\n",
    "\n",
    "full_train_ds= SequenceImageDataset(full_df[full_df['date_value'] <= max_train_date], img_list)\n",
    "full_test_ds = SequenceImageDataset(full_df[full_df['date_value'] > max_train_date], img_list)\n",
    "\n",
    "full_train_dl = DataLoader(full_train_ds, batch_size = CFG.batch_size, shuffle=True)\n",
    "full_test_dl = DataLoader(full_test_ds, batch_size = CFG.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e45a6c",
   "metadata": {
    "papermill": {
     "duration": 0.007262,
     "end_time": "2025-04-11T16:22:48.651646",
     "exception": false,
     "start_time": "2025-04-11T16:22:48.644384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ablation Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fafa151e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:48.667668Z",
     "iopub.status.busy": "2025-04-11T16:22:48.667381Z",
     "iopub.status.idle": "2025-04-11T16:22:48.675280Z",
     "shell.execute_reply": "2025-04-11T16:22:48.674444Z"
    },
    "papermill": {
     "duration": 0.017262,
     "end_time": "2025-04-11T16:22:48.676501",
     "exception": false,
     "start_time": "2025-04-11T16:22:48.659239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "\n",
    "def run_grid_search(\n",
    "    param_grid: dict,\n",
    "        df_dict,\n",
    "        model_name,\n",
    "    model_save_dir: str,\n",
    "    result_json_path: str,\n",
    "    default_params: dict = None\n",
    "):\n",
    "    from copy import deepcopy\n",
    "\n",
    "    # Generate all parameter combinations\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    \n",
    "    param_combinations = [\n",
    "        {**(default_params or {}), **dict(zip(keys, v))}\n",
    "        for v in itertools.product(*values)\n",
    "    ]\n",
    "    \n",
    "    model_dict = {}\n",
    "    results_list = []\n",
    "    \n",
    "    for idx, param_set in enumerate(param_combinations):\n",
    "        print(f\"Training model {idx} with params: {param_set}\")\n",
    "        \n",
    "        model = get_model(**param_set)\n",
    "        criterion = MaskedMSELoss().to(device)\n",
    "        optimizer = get_optimizer(model, lr = CFG.lr)\n",
    "        scheduler = get_reduce_lr(optimizer, factor=CFG.factor, patience=CFG.patience)\n",
    "        \n",
    "        loss_list, val_loss_list, all_loss_list = train(\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            num_epochs=CFG.num_epochs,\n",
    "            train_dataloader=df_dict['train_dl'],\n",
    "            val_dataloader=df_dict['test_dl'],\n",
    "            scheduler=scheduler,\n",
    "            do_validate=True,\n",
    "            validate_frequency=1,\n",
    "            verbose=-2,\n",
    "            model_name=model_save_dir + f\"{model_name}_{idx}_\"\n",
    "        )\n",
    "        \n",
    "        model_path = model_save_dir + f\"final_{model_name}_{idx}.pt\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        model_dict[idx] = model\n",
    "\n",
    "        result_metrics = {}\n",
    "        paths = ['best', '2_5', '10']\n",
    "        for p in paths:\n",
    "            result_metrics[p] = {}\n",
    "            model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n",
    "            model.load_state_dict(model_state_dict)\n",
    "    \n",
    "            for df_name, df1 in df_dict.items():\n",
    "                result_metrics[p][df_name] = predict_eval(model, df1, criterion, verbose=False)\n",
    "\n",
    "        results_list.append({\n",
    "            \"model_index\": idx,\n",
    "            \"model_name\": model_path,\n",
    "            \"parameters\": deepcopy(param_set),\n",
    "            \"results\": result_metrics\n",
    "        })\n",
    "\n",
    "    with open(result_json_path, \"w\") as f:\n",
    "        json.dump(results_list, f, indent=4)\n",
    "\n",
    "    return model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd61d5ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:22:48.691920Z",
     "iopub.status.busy": "2025-04-11T16:22:48.691665Z",
     "iopub.status.idle": "2025-04-11T16:35:31.968027Z",
     "shell.execute_reply": "2025-04-11T16:35:31.967317Z"
    },
    "papermill": {
     "duration": 763.285672,
     "end_time": "2025-04-11T16:35:31.969614",
     "exception": false,
     "start_time": "2025-04-11T16:22:48.683942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 0 with params: {'input_size': 10, 'forecast_horizon': 35, 'hidden_size': 64, 'num_layers': 1, 'model_kind': 'rnn'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n",
      "100%|██████████| 50/50 [01:16<00:00,  1.52s/it]\n",
      "<ipython-input-29-c971aca37c8d>:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n",
      "<ipython-input-24-4e72c1729806>:2: UserWarning: torch.searchsorted(): input value tensor is non-contiguous, this will lower the performance due to extra data copy when converting non-contiguous tensor to contiguous, please use contiguous input value tensor if possible. This message will only appear once per program. (Triggered internally at ../aten/src/ATen/native/BucketizationUtils.h:32.)\n",
      "  indices = torch.bucketize(preds, torch.tensor(thresholds).to(device), right=False) - 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1 with params: {'input_size': 10, 'forecast_horizon': 35, 'hidden_size': 64, 'num_layers': 1, 'model_kind': 'gru'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:17<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 2 with params: {'input_size': 10, 'forecast_horizon': 35, 'hidden_size': 64, 'num_layers': 3, 'model_kind': 'rnn'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:17<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 3 with params: {'input_size': 10, 'forecast_horizon': 35, 'hidden_size': 64, 'num_layers': 3, 'model_kind': 'gru'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:20<00:00,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 4 with params: {'input_size': 10, 'forecast_horizon': 35, 'hidden_size': 64, 'num_layers': 6, 'model_kind': 'rnn'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:22<00:00,  1.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 5 with params: {'input_size': 10, 'forecast_horizon': 35, 'hidden_size': 64, 'num_layers': 6, 'model_kind': 'gru'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:28<00:00,  1.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 6 with params: {'input_size': 10, 'forecast_horizon': 35, 'hidden_size': 64, 'num_layers': 9, 'model_kind': 'rnn'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:26<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 7 with params: {'input_size': 10, 'forecast_horizon': 35, 'hidden_size': 64, 'num_layers': 9, 'model_kind': 'gru'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:38<00:00,  1.97s/it]\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    # 'hidden_size' : [16, 32, 64, 128], \n",
    "    'hidden_size' : [64], \n",
    "    'num_layers' : [1, 3, 6, 9],\n",
    "    'model_kind': ['rnn', 'gru']\n",
    "}\n",
    "\n",
    "default_params = {\n",
    "    'input_size' : 10, \n",
    "    'forecast_horizon' : 35\n",
    "}\n",
    "\n",
    "df_dict = {\n",
    "    'train_dl' : train_dl,\n",
    "    'test_dl' : test_dl,\n",
    "    'full_train_dl' : full_train_dl,\n",
    "    'full_test_dl' : full_test_dl,\n",
    "}\n",
    "\n",
    "model_name = \"model_size_test\"\n",
    "\n",
    "model_save_dir = f\"./{model_name}_dir/\"\n",
    "result_json_path = f\"{model_save_dir}{model_name}_results.json\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "model_dict = run_grid_search(\n",
    "    param_grid,\n",
    "        df_dict,\n",
    "        model_name,\n",
    "    model_save_dir,\n",
    "    result_json_path,\n",
    "    default_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1daff4",
   "metadata": {
    "papermill": {
     "duration": 0.025993,
     "end_time": "2025-04-11T16:35:32.022217",
     "exception": false,
     "start_time": "2025-04-11T16:35:31.996224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ablation 2: Input Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecc96396",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:35:32.074995Z",
     "iopub.status.busy": "2025-04-11T16:35:32.074589Z",
     "iopub.status.idle": "2025-04-11T16:35:32.078560Z",
     "shell.execute_reply": "2025-04-11T16:35:32.077859Z"
    },
    "papermill": {
     "duration": 0.031868,
     "end_time": "2025-04-11T16:35:32.079815",
     "exception": false,
     "start_time": "2025-04-11T16:35:32.047947",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dl(df, sequence_length=35, forecast_horizon=35):\n",
    "    ds = SequenceImageDataset(df, \n",
    "                              sequence_length = sequence_length, forecast_horizon = forecast_horizon,\n",
    "                             img_list=img_list)\n",
    "    dl = DataLoader(ds, batch_size = CFG.batch_size, shuffle=True)\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eeeceb78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:35:32.132126Z",
     "iopub.status.busy": "2025-04-11T16:35:32.131875Z",
     "iopub.status.idle": "2025-04-11T16:35:32.140143Z",
     "shell.execute_reply": "2025-04-11T16:35:32.139516Z"
    },
    "papermill": {
     "duration": 0.035622,
     "end_time": "2025-04-11T16:35:32.141356",
     "exception": false,
     "start_time": "2025-04-11T16:35:32.105734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_grid_search_df(\n",
    "    param_grid: dict,\n",
    "        df_dict,\n",
    "        model_name,\n",
    "    model_save_dir: str,\n",
    "    result_json_path: str,\n",
    "    default_params: dict = None,\n",
    "    model_params: dict = None,\n",
    "):\n",
    "    from copy import deepcopy\n",
    "\n",
    "    # Generate all parameter combinations\n",
    "    keys, values = zip(*param_grid.items())\n",
    "    \n",
    "    param_combinations = [\n",
    "        {**(default_params or {}), **dict(zip(keys, v))}\n",
    "        for v in itertools.product(*values)\n",
    "    ]\n",
    "    \n",
    "    model_dict = {}\n",
    "    results_list = []\n",
    "    \n",
    "    for idx, param_set in enumerate(param_combinations):\n",
    "        print(f\"Training model on dataset {idx} with params: {param_set}\")\n",
    "\n",
    "        model = get_model(**model_params)\n",
    "\n",
    "        ds_dict = {\n",
    "            k : get_dl(df = v, **param_set)\n",
    "            for k, v in df_dict.items()\n",
    "        }\n",
    "        \n",
    "        criterion = MaskedMSELoss().to(device)\n",
    "        optimizer = get_optimizer(model, lr = CFG.lr)\n",
    "        scheduler = get_reduce_lr(optimizer, factor=CFG.factor, patience=CFG.patience)\n",
    "        \n",
    "        loss_list, val_loss_list, all_loss_list = train(\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            num_epochs=CFG.num_epochs,\n",
    "            train_dataloader=ds_dict['train_dl'],\n",
    "            val_dataloader=ds_dict['test_dl'],\n",
    "            scheduler=scheduler,\n",
    "            do_validate=True,\n",
    "            validate_frequency=1,\n",
    "            verbose=-2,\n",
    "            model_name=model_save_dir + f\"{model_name}_{idx}_\"\n",
    "        )\n",
    "        \n",
    "        model_path = model_save_dir + f\"final_{model_name}_{idx}.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        model_dict[idx] = model\n",
    "        \n",
    "        result_metrics = {}\n",
    "        paths = ['best', '2_5', '10']\n",
    "        for p in paths:\n",
    "            result_metrics[p] = {}\n",
    "            model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            \n",
    "            for df_name, df1 in ds_dict.items():\n",
    "                result_metrics[p][df_name] = predict_eval(model, df1, criterion, verbose=False)\n",
    "\n",
    "        results_list.append({\n",
    "            \"model_index\": idx,\n",
    "            \"model_name\": model_path,\n",
    "            \"parameters\": deepcopy(param_set),\n",
    "            \"results\": result_metrics\n",
    "        })\n",
    "\n",
    "    with open(result_json_path, \"w\") as f:\n",
    "        json.dump(results_list, f, indent=4)\n",
    "\n",
    "    return model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f229eb1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:35:32.193574Z",
     "iopub.status.busy": "2025-04-11T16:35:32.193273Z",
     "iopub.status.idle": "2025-04-11T16:43:39.970125Z",
     "shell.execute_reply": "2025-04-11T16:43:39.969263Z"
    },
    "papermill": {
     "duration": 487.805012,
     "end_time": "2025-04-11T16:43:39.972100",
     "exception": false,
     "start_time": "2025-04-11T16:35:32.167088",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 0 with params: {'sequence_length': 35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:18<00:00,  1.57s/it]\n",
      "<ipython-input-32-03f43a5151d9>:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 1 with params: {'sequence_length': 70}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:18<00:00,  1.58s/it]\n",
      "<ipython-input-32-03f43a5151d9>:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 2 with params: {'sequence_length': 105}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:18<00:00,  1.58s/it]\n",
      "<ipython-input-32-03f43a5151d9>:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 3 with params: {'sequence_length': 140}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:18<00:00,  1.57s/it]\n",
      "<ipython-input-32-03f43a5151d9>:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 4 with params: {'sequence_length': 175}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:17<00:00,  1.55s/it]\n",
      "<ipython-input-32-03f43a5151d9>:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'sequence_length' : [35, 70, 105, 140, 175], \n",
    "}\n",
    "\n",
    "default_params = {\n",
    "    'input_size' : 10, \n",
    "    'forecast_horizon' : 35\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    'input_size': 10,\n",
    "    'forecast_horizon': 35,\n",
    "    'num_layers': 3,\n",
    "    'hidden_size': 64,\n",
    "    'model_kind': 'rnn'\n",
    "}\n",
    "\n",
    "df_dict = {\n",
    "    'train_dl' : df[df['date_value'] <= max_train_date],\n",
    "    'test_dl' : df[df['date_value'] > max_train_date],\n",
    "    'full_train_dl' : full_df[full_df['date_value'] <= max_train_date],\n",
    "    'full_test_dl' : full_df[full_df['date_value'] > max_train_date],\n",
    "}\n",
    "\n",
    "model_name = \"rnn_sequence_length_test\"\n",
    "\n",
    "model_save_dir = f\"./{model_name}_dir/\"\n",
    "result_json_path = f\"{model_save_dir}{model_name}_results.json\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "model_dict1 = run_grid_search_df(\n",
    "    param_grid,\n",
    "        df_dict,\n",
    "        model_name,\n",
    "    model_save_dir,\n",
    "    result_json_path,\n",
    "    default_params=None,\n",
    "    model_params = model_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32e3750c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:43:40.050644Z",
     "iopub.status.busy": "2025-04-11T16:43:40.050310Z",
     "iopub.status.idle": "2025-04-11T16:52:09.884021Z",
     "shell.execute_reply": "2025-04-11T16:52:09.883140Z"
    },
    "papermill": {
     "duration": 509.874044,
     "end_time": "2025-04-11T16:52:09.885721",
     "exception": false,
     "start_time": "2025-04-11T16:43:40.011677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 0 with params: {'sequence_length': 35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:20<00:00,  1.60s/it]\n",
      "<ipython-input-32-03f43a5151d9>:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 1 with params: {'sequence_length': 70}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:22<00:00,  1.64s/it]\n",
      "<ipython-input-32-03f43a5151d9>:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 2 with params: {'sequence_length': 105}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:23<00:00,  1.67s/it]\n",
      "<ipython-input-32-03f43a5151d9>:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 3 with params: {'sequence_length': 140}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:24<00:00,  1.69s/it]\n",
      "<ipython-input-32-03f43a5151d9>:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 4 with params: {'sequence_length': 175}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:25<00:00,  1.70s/it]\n",
      "<ipython-input-32-03f43a5151d9>:59: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'input_size': 10,\n",
    "    'forecast_horizon': 35,\n",
    "    'num_layers': 3,\n",
    "    'hidden_size': 64,\n",
    "    'model_kind': 'gru'\n",
    "}\n",
    "\n",
    "df_dict = {\n",
    "    'train_dl' : df[df['date_value'] <= max_train_date],\n",
    "    'test_dl' : df[df['date_value'] > max_train_date],\n",
    "    'full_train_dl' : full_df[full_df['date_value'] <= max_train_date],\n",
    "    'full_test_dl' : full_df[full_df['date_value'] > max_train_date],\n",
    "}\n",
    "\n",
    "model_name = \"gru_sequence_length_test\"\n",
    "\n",
    "model_save_dir = f\"./{model_name}_dir/\"\n",
    "result_json_path = f\"{model_save_dir}{model_name}_results.json\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "model_dict2 = run_grid_search_df(\n",
    "    param_grid,\n",
    "        df_dict,\n",
    "        model_name,\n",
    "    model_save_dir,\n",
    "    result_json_path,\n",
    "    default_params=None,\n",
    "    model_params = model_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d7996",
   "metadata": {
    "papermill": {
     "duration": 0.050438,
     "end_time": "2025-04-11T16:52:09.986992",
     "exception": false,
     "start_time": "2025-04-11T16:52:09.936554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Ablation 3: Forecast Horizon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d5690be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:52:10.087267Z",
     "iopub.status.busy": "2025-04-11T16:52:10.086928Z",
     "iopub.status.idle": "2025-04-11T16:52:10.094878Z",
     "shell.execute_reply": "2025-04-11T16:52:10.094161Z"
    },
    "papermill": {
     "duration": 0.05997,
     "end_time": "2025-04-11T16:52:10.096163",
     "exception": false,
     "start_time": "2025-04-11T16:52:10.036193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_grid_search_horizon(\n",
    "    horizon_values,\n",
    "        df_dict,\n",
    "        model_name,\n",
    "    model_save_dir: str,\n",
    "    result_json_path: str,\n",
    "    df_params: dict = None,\n",
    "    model_params: dict = None,\n",
    "):    \n",
    "    model_dict = {}\n",
    "    results_list = []\n",
    "    \n",
    "    for idx, h in enumerate(horizon_values):\n",
    "        print(f\"Training model on dataset {idx} with horizon: {h}\")\n",
    "\n",
    "        model = get_model(forecast_horizon=h, **model_params)\n",
    "\n",
    "        ds_dict = {\n",
    "            k : get_dl(df = v, forecast_horizon = h, **df_params)\n",
    "            for k, v in df_dict.items()\n",
    "        }\n",
    "        \n",
    "        criterion = MaskedMSELoss().to(device)\n",
    "        optimizer = get_optimizer(model, lr = CFG.lr)\n",
    "        scheduler = get_reduce_lr(optimizer, factor=CFG.factor, patience=CFG.patience)\n",
    "        \n",
    "        loss_list, val_loss_list, all_loss_list = train(\n",
    "            model,\n",
    "            criterion,\n",
    "            optimizer,\n",
    "            num_epochs=CFG.num_epochs,\n",
    "            train_dataloader=ds_dict['train_dl'],\n",
    "            val_dataloader=ds_dict['test_dl'],\n",
    "            scheduler=scheduler,\n",
    "            do_validate=True,\n",
    "            validate_frequency=1,\n",
    "            verbose=-2,\n",
    "            model_name=model_save_dir + f\"{model_name}_{idx}_\"\n",
    "        )\n",
    "        \n",
    "        model_path = model_save_dir + f\"final_{model_name}_{idx}.pt\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        model_dict[idx] = model\n",
    "        \n",
    "        result_metrics = {}\n",
    "        paths = ['best', '2_5', '10']\n",
    "        for p in paths:\n",
    "            result_metrics[p] = {}\n",
    "            model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n",
    "            model.load_state_dict(model_state_dict)\n",
    "            \n",
    "            for df_name, df1 in ds_dict.items():\n",
    "                result_metrics[p][df_name] = predict_eval(model, df1, criterion, verbose=False)\n",
    "\n",
    "        results_list.append({\n",
    "            \"model_index\": idx,\n",
    "            \"model_name\": model_path,\n",
    "            \"horizon\": h,\n",
    "            \"params\" : model_params,\n",
    "            \"results\": result_metrics\n",
    "        })\n",
    "\n",
    "    with open(result_json_path, \"w\") as f:\n",
    "        json.dump(results_list, f, indent=4)\n",
    "\n",
    "    return model_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3f57c45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T16:52:10.196025Z",
     "iopub.status.busy": "2025-04-11T16:52:10.195662Z",
     "iopub.status.idle": "2025-04-11T17:00:00.578516Z",
     "shell.execute_reply": "2025-04-11T17:00:00.577816Z"
    },
    "papermill": {
     "duration": 470.434279,
     "end_time": "2025-04-11T17:00:00.580106",
     "exception": false,
     "start_time": "2025-04-11T16:52:10.145827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 0 with horizon: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:18<00:00,  1.57s/it]\n",
      "<ipython-input-35-c7319f056be7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 1 with horizon: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:17<00:00,  1.55s/it]\n",
      "<ipython-input-35-c7319f056be7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 2 with horizon: 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:15<00:00,  1.51s/it]\n",
      "<ipython-input-35-c7319f056be7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 3 with horizon: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:13<00:00,  1.48s/it]\n",
      "<ipython-input-35-c7319f056be7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 4 with horizon: 175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:12<00:00,  1.45s/it]\n",
      "<ipython-input-35-c7319f056be7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    }
   ],
   "source": [
    "horizon_values = [35, 70, 105, 140, 175]\n",
    "\n",
    "model_params = {\n",
    "    'input_size': 10,\n",
    "    'num_layers': 3,\n",
    "    'hidden_size': 64,\n",
    "    'model_kind': 'rnn'\n",
    "}\n",
    "\n",
    "df_dict = {\n",
    "    'train_dl' : df[df['date_value'] <= max_train_date],\n",
    "    'test_dl' : df[df['date_value'] > max_train_date],\n",
    "    'full_train_dl' : full_df[full_df['date_value'] <= max_train_date],\n",
    "    'full_test_dl' : full_df[full_df['date_value'] > max_train_date],\n",
    "}\n",
    "\n",
    "df_params = {\n",
    "    'sequence_length' : 105,\n",
    "}\n",
    "\n",
    "model_name = \"rnn_forecast_horizon_test\"\n",
    "\n",
    "model_save_dir = f\"./{model_name}_dir/\"\n",
    "result_json_path = f\"{model_save_dir}{model_name}_results.json\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "model_dict3 = run_grid_search_horizon(\n",
    "    horizon_values,\n",
    "        df_dict,\n",
    "        model_name,\n",
    "    model_save_dir,\n",
    "    result_json_path,\n",
    "    df_params=df_params,\n",
    "    model_params = model_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e4d115e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T17:00:00.704597Z",
     "iopub.status.busy": "2025-04-11T17:00:00.704280Z",
     "iopub.status.idle": "2025-04-11T17:08:15.257182Z",
     "shell.execute_reply": "2025-04-11T17:08:15.256345Z"
    },
    "papermill": {
     "duration": 494.617153,
     "end_time": "2025-04-11T17:08:15.258782",
     "exception": false,
     "start_time": "2025-04-11T17:00:00.641629",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 0 with horizon: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:23<00:00,  1.66s/it]\n",
      "<ipython-input-35-c7319f056be7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 1 with horizon: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:22<00:00,  1.65s/it]\n",
      "<ipython-input-35-c7319f056be7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 2 with horizon: 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:19<00:00,  1.60s/it]\n",
      "<ipython-input-35-c7319f056be7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 3 with horizon: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:18<00:00,  1.58s/it]\n",
      "<ipython-input-35-c7319f056be7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model on dataset 4 with horizon: 175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [01:16<00:00,  1.54s/it]\n",
      "<ipython-input-35-c7319f056be7>:49: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(model_save_dir + f\"{model_name}_{idx}_{p}.pth\")\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'input_size': 10,\n",
    "    'num_layers': 3,\n",
    "    'hidden_size': 64,\n",
    "    'model_kind': 'gru'\n",
    "}\n",
    "\n",
    "df_dict = {\n",
    "    'train_dl' : df[df['date_value'] <= max_train_date],\n",
    "    'test_dl' : df[df['date_value'] > max_train_date],\n",
    "    'full_train_dl' : full_df[full_df['date_value'] <= max_train_date],\n",
    "    'full_test_dl' : full_df[full_df['date_value'] > max_train_date],\n",
    "}\n",
    "\n",
    "df_params = {\n",
    "    'sequence_length' : 105,\n",
    "}\n",
    "\n",
    "model_name = \"gru_forecast_horizon_test\"\n",
    "\n",
    "model_save_dir = f\"./{model_name}_dir/\"\n",
    "result_json_path = f\"{model_save_dir}{model_name}_results.json\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "model_dict3 = run_grid_search_horizon(\n",
    "    horizon_values,\n",
    "        df_dict,\n",
    "        model_name,\n",
    "    model_save_dir,\n",
    "    result_json_path,\n",
    "    df_params=df_params,\n",
    "    model_params = model_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5a2c7",
   "metadata": {
    "papermill": {
     "duration": 0.071827,
     "end_time": "2025-04-11T17:08:15.406731",
     "exception": false,
     "start_time": "2025-04-11T17:08:15.334904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11553091,
     "datasetId": 6950669,
     "sourceId": 11154655,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 226700844,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 228118212,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 232490461,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2776.028311,
   "end_time": "2025-04-11T17:08:18.307955",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-11T16:22:02.279644",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
